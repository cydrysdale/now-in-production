<!doctype html>
<html lang="en">

<head>
  <script>
    try {
      const t = localStorage.getItem('theme');
      if (t === 'dark' || t === 'light') document.documentElement.setAttribute('data-theme', t);
    } catch {}
  </script>
  <meta charset="utf-8" />
  <title>Demystifying AI: Stepping into the LLM Kitchen</title>
  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16.png">
  <link rel="icon" href="../images/favicon.ico" type="image/x-icon">
  <link rel="manifest" href="../site.webmanifest">
  <meta name="description" content="A practical guide to how LLMs work—using a pixel kitchen/restaurant sim metaphor to teach prompts, context windows, tools, QA, and agent workflows." />
  <meta name="keywords" content="LLMs, large language models, how ChatGPT works, tokens, context window, attention, prompting, prompt engineering, decoding, temperature, top-p, hallucinations, AI tools, citations, verification, QA checklist, agents" />
  <meta name="guide:category" content="AI" />
  <meta name="guide:updated" content="2026-02-27" />
  <link rel="canonical" href="https://cydrysdale.github.io/now-in-production/guides/how-llms-work.html">
  <meta name="robots" content="index,follow">
  <meta name="author" content="Chris Yasuda Drysdale">
  <meta name="format-detection" content="telephone=no" />
  <meta property="og:type" content="article">
  <meta property="og:title" content="Demystifying AI: Stepping into the LLM Kitchen">
  <meta property="og:description" content="A playful, practical guide for using LLMs reliably: tickets, prep counters, tools, expo pass QA, and station workflows." />
  <meta property="og:image" content="https://cydrysdale.github.io/now-in-production/images/how-llms-work-card.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta property="og:url" content="https://cydrysdale.github.io/now-in-production/guides/how-llms-work.html">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Fredericka+the+Great&amp;family=VT323&amp;display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../assets/css/style.css">
  <link rel="stylesheet" href="../assets/css/tokens.css">
  <script src="../assets/js/core.js" defer></script>
  <script src="../assets/js/widgets.js" defer></script>
  <script src="../assets/js/tokens.js" defer></script>
  <style>
    :root {
      /* NOTE: url(...) is resolved relative to `assets/css/style.css` (not this HTML file). */
      --bg-light: url('../../images/how-llms-work-bg-lt.png');
    }
    body {
      background-position: center;
      background-attachment: fixed;
    }
    :root[data-theme="dark"]{ 
      --bg-dark: url('../../images/how-llms-work-bg-dk.png')
    }

    .badge {
      display: inline-block;
      padding: 2px 10px;
      border-radius: 999px;
      border: 1px solid var(--borderDark);
      background: var(--card2);
      font-size: .85rem;
      color: var(--muted);
      vertical-align: middle;
    }

    .hero {
      display: grid;
      grid-template-columns: 1.15fr .85fr;
      gap: 16px;
      align-items: start;
    }

    @media (max-width: 980px) {
      .hero { grid-template-columns: 1fr; }
    }

    .kicker { font-size: 1.05rem; }
    .note { font-size: .95rem; color: var(--muted); }

    .truth {
      margin: 10px 0 16px;
      padding: 10px 12px;
      border: 1px solid var(--borderLight);
      border-left: 4px solid var(--accent);
      border-radius: 10px;
      background: linear-gradient(30deg, var(--callout), var(--card));
      font-style: italic;
    }

    .truth strong { font-style: normal; }

    .diagram {
      border: 1px solid var(--borderDark);
      border-radius: 14px;
      background: var(--card);
      overflow: hidden;
    }

    .diagram figcaption {
      padding: 10px 12px;
      border-top: 1px solid var(--borderDark);
      color: var(--muted);
      font-size: .95rem;
    }

    .diagram img { width: 100%; height: auto; display: block; }

    .sandbox-input.wide { width: min(760px, 100%); }
    textarea.sandbox-input { width: min(760px, 100%); min-height: 110px; }

    .roster {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 14px;
    }

    @media (max-width: 1100px) { .roster { grid-template-columns: repeat(2, 1fr); } }
    @media (max-width: 700px) { .roster { grid-template-columns: 1fr; } }

    .roster .chef {
      border: 1px solid var(--borderDark);
      border-radius: 14px;
      background: var(--card);
      overflow: hidden;
    }
    .roster .chef .body { padding: 12px 12px 2px; }
    .roster .chef img { width: 100%; height: auto; display: block; }
    .roster .chef h3 { margin-top: 0; }

    .quest input[type="checkbox"] { transform: translateY(1px); margin-right: 8px; }
    .quest li { margin-bottom: 8px; }

    #pgChoices { display: flex; gap: 8px; padding: 8px 0; }
    #pgChoices button { flex: 1; min-width: 80px; }
    #pgChoices button:disabled { cursor: default; }

    .kitchen-scene {
      position: relative;
      width: 100%;
      aspect-ratio: 2 / 1;
      background: #8b7355 url('../images/kitchen-prep-counter.png') center / cover no-repeat;
      image-rendering: pixelated;
      -webkit-image-rendering: crisp-edges;
      border-radius: 10px;
      border: 1px solid var(--borderDark);
      margin-bottom: 10px;
      overflow: hidden;
    }

    .kitchen-ticket {
      position: absolute;
      top: 6%;
      width: 10%;
      aspect-ratio: 1 / 1;
      background-size: contain;
      background-repeat: no-repeat;
      background-position: center;
      display: flex;
      align-items: center;
      justify-content: center;
      animation: ticketDropIn 0.35s ease-out both;
    }

    .kitchen-ticket[data-settled] {
      transition: left 0.3s ease;
    }

    .kitchen-ticket--prompt {
      background-image: url('../images/kitchen-ticket-prompt.png');
    }

    .kitchen-ticket--msg {
      background-image: url('../images/kitchen-ticket-msg.png');
    }

    .kitchen-ticket__label {
      font-size: 0.9rem;
      font-weight: bold;
      color: #352f36;
      text-align: center;
      line-height: 1.2;
      padding: 2px;
      pointer-events: none;
      margin-top: 10%;
    }

    .kitchen-ticket--falling {
      animation: ticketFallOff 1.2s ease-in forwards !important;
      transition: none !important;
    }

    @keyframes ticketDropIn {
      0%   { transform: translateY(-120%); opacity: 0; }
      60%  { transform: translateY(8%); opacity: 1; }
      80%  { transform: translateY(-3%); }
      100% { transform: translateY(0); opacity: 1; }
    }

    @keyframes ticketFallOff {
      0%   { transform: translateY(0) rotate(0deg); opacity: 1; }
      20%  { transform: translateY(30%) rotate(-8deg); opacity: 1; }
      40%  { transform: translateY(80%) rotate(5deg); opacity: 0.9; }
      60%  { transform: translateY(150%) rotate(-10deg); opacity: 0.7; }
      80%  { transform: translateY(250%) rotate(6deg); opacity: 0.4; }
      100% { transform: translateY(400%) rotate(-12deg); opacity: 0; }
    }

    @media (max-width: 560px) {
      .kitchen-ticket__label { display: none; }
      .kitchen-ticket { width: 11%; }
    }

    @media (prefers-reduced-motion: reduce) {
      .kitchen-ticket { animation: none !important; }
      .kitchen-ticket--falling { animation: none !important; opacity: 0; }
    }

    #hqChoices { display: flex; gap: 8px; padding: 8px 0; }

    /* Chef Select */
    #chefSelect { --chef-accent: var(--accent); }
    .select-screen { display: flex; flex-direction: column; gap: 12px; }
    .portrait-row { display: grid; grid-template-columns: repeat(6, 1fr); gap: 8px; }
    @media (max-width: 600px) { .portrait-row { grid-template-columns: repeat(3, 1fr); } }

    .portrait {
      aspect-ratio: 1;
      border: 2px solid var(--borderDark);
      border-radius: 8px;
      background: var(--card2);
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: .8rem;
      font-weight: bold;
      color: var(--muted);
      transition: border-color 0.15s, box-shadow 0.15s;
    }

    .portrait:hover { border-color: var(--chef-accent); }
    .portrait.active { border-color: var(--chef-accent); box-shadow: 0 0 0 2px var(--chef-accent); }
    .portrait img { width: 100%; height: 100%; object-fit: cover; border-radius: 6px; }

    .chef-panel {
      display: grid;
      grid-template-columns: 240px 1fr;
      border: 1px solid var(--chef-accent);
      border-radius: 14px;
      background: var(--card);
      overflow: hidden;
      box-shadow: 0 0 0 1px color-mix(in srgb, var(--chef-accent) 26%, transparent);
      transition: border-color 0.24s ease, box-shadow 0.24s ease;
    }

    @media (max-width: 700px) { .chef-panel { grid-template-columns: 1fr; } }

    .chef-art {
      background: var(--card2);
      display: flex;
      align-items: flex-end;
      justify-content: center;
    }

    .chef-art img { height: auto; display: block; }
    .chef-art img.chef-art-img { width: 100%; }
    .chef-art img.chef-art-placeholder { width: 100%; opacity: .7; }
    .chef-info { padding: 16px 20px; }
    .chef-name { font-size: 1.3rem; font-weight: bold; margin: 0 0 2px; }
    .chef-title-label { color: var(--chef-accent); font-weight: bold; font-size: .9rem; margin-bottom: 10px; }
    .chef-bio { font-size: .9rem; line-height: 1.5; margin-bottom: 14px; }
    .chef-stats { display: flex; flex-direction: column; gap: 3px; }
    .stat-row { display: grid; grid-template-columns: 40px minmax(0, 1fr); align-items: center; gap: 8px; font-size: .85rem; }
    .stat-label { font-weight: bold; width: 32px; font-size: .8rem; }
    .stat-stars { letter-spacing: 2px; white-space: nowrap; font-size: .92rem; line-height: 1; }
    .stat-stars .star-on { color: var(--chef-accent); }
    .stat-stars .star-off { color: color-mix(in srgb, var(--chef-accent) 28%, var(--muted)); }

    @media (prefers-reduced-motion: no-preference) {
      #chefSelect.motion-ready {
        opacity: 0;
        transform: translateY(20px);
        transition: opacity 0.6s ease, transform 0.6s ease;
      }
      #chefSelect.motion-ready.in-view {
        opacity: 1;
        transform: translateY(0);
      }
      #chefSelect.motion-ready.in-view.exiting-up {
        opacity: 0.5;
        transform: translateY(-10px) scale(0.99);
      }

      .portrait {
        transition: border-color 0.15s, box-shadow 0.15s, transform 0.2s ease, filter 0.2s ease;
      }
      .portrait:hover {
        transform: scale(1.06) translateY(-2px);
        filter: brightness(1.08);
      }
      .portrait.hint-pulse { animation: portraitHint 1.15s ease-in-out 2; }

      @keyframes portraitHint {
        0%, 100% { transform: translateY(0) scale(1); box-shadow: 0 0 0 0 var(--chef-accent); }
        50% { transform: translateY(-3px) scale(1.04); box-shadow: 0 0 0 3px color-mix(in srgb, var(--chef-accent) 40%, transparent); }
      }

      .chef-art, .chef-info {
        transition: opacity 0.2s ease, transform 0.2s ease;
      }
      .chef-art.flipping {
        opacity: 0;
        transform: perspective(700px) rotateY(14deg) scale(0.98);
      }
      .chef-info.swapping {
        opacity: 0;
        transform: translateY(8px);
      }
      .chef-panel.accent-pulse {
        box-shadow: 0 0 0 2px var(--chef-accent), 0 8px 26px color-mix(in srgb, var(--chef-accent) 35%, transparent);
      }
      @keyframes chefFloat {
        0%, 100% { transform: translateY(0); }
        50% { transform: translateY(-3px); }
      }
      .chef-art img.chef-art-img {
        animation: chefFloat 4s ease-in-out infinite;
      }
    }

    /* Wireframe demo (appendix)—overrides for placeholder variant */
    #tplChefSelect { --chef-accent: var(--accent); }
    .wireframe-demo .chef-art {
      align-items: center;
      padding: 16px;
      min-height: 240px;
    }
    .wireframe-demo .chef-art-placeholder {
      width: 100%;
      min-height: 208px;
      border-radius: 10px;
      border: 1px dashed var(--borderDark);
      display: flex;
      align-items: center;
      justify-content: center;
      color: var(--muted);
      font-weight: 800;
      letter-spacing: .06em;
      text-transform: uppercase;
      user-select: none;
    }

    /* Sprite progression carousel */
    .sprite-carousel {
      position: relative;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .roster-carousel {
      display: flex;
      overflow-x: auto;
      scroll-snap-type: x mandatory;
      scroll-behavior: smooth;
      -webkit-overflow-scrolling: touch;
      scrollbar-width: none;
      flex: 1;
      min-width: 0;
      mask-image: linear-gradient(to right, transparent, black 15%, black 85%, transparent);
      -webkit-mask-image: linear-gradient(to right, transparent, black 15%, black 85%, transparent);
    }
    .roster-carousel::-webkit-scrollbar { display: none; }
    .roster-carousel > .diagram {
      flex: 0 0 60%;
      scroll-snap-align: center;
      margin: 0;
    }
    .roster-carousel > .diagram img {
      width: 75%;
      margin: 0 auto;
    }
    /* Pad first/last so they can center */
    .roster-carousel::before,
    .roster-carousel::after {
      content: '';
      flex: 0 0 20%;
    }

    .carousel-arrow {
      flex-shrink: 0;
      width: 36px;
      height: 36px;
      border-radius: 50%;
      border: 1.5px solid var(--borderDark);
      background: var(--card);
      color: var(--text);
      font-size: 1.1rem;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: border-color 0.2s, background 0.2s;
      z-index: 1;
    }
    .carousel-arrow:hover {
      border-color: var(--accent);
      background: var(--card2);
    }
    .carousel-arrow:disabled {
      opacity: 0.3;
      cursor: default;
    }

    .carousel-dots {
      display: flex;
      justify-content: center;
      gap: 8px;
      padding: 10px 0 2px;
    }
    .carousel-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      border: 1.5px solid var(--muted);
      background: transparent;
      padding: 0;
      cursor: pointer;
      transition: background 0.2s, border-color 0.2s;
    }
    .carousel-dot.active {
      background: var(--accent);
      border-color: var(--accent);
    }

    @media (max-width: 700px) {
      .carousel-arrow { display: none; }
      .roster-carousel > .diagram { flex: 0 0 85%; }
      .roster-carousel::before,
      .roster-carousel::after { flex: 0 0 7.5%; }
    }

    @media (prefers-reduced-motion: reduce) {
      .roster-carousel { scroll-behavior: auto; }
    }
  </style>
</head>

<body>
  <a class="skip-link" href="#content">Skip to content</a>
  <header>
    <div class="header-inner">
      <img src="../images/Pixel-Chris.png" width="60" style="display: inline-block; margin-right: 10px;"
        alt="site logo" />
      <div id="top" class="title-block">
        <h1>Demystifying AI: Stepping into the LLM Kitchen</h1>
        <div class="sub">A basic guide on how LLMs work for the extremely nerdy</div>
      </div>
      <div class="header-actions">
        <div class="theme-toggle" style="scale: 75%;">
          <input id="themeSwitch" type="checkbox" role="switch" aria-checked="false" aria-label="Toggle dark mode">
          <label for="themeSwitch">
            <span class="track" aria-hidden="true"></span>
            <span class="thumb" aria-hidden="true">
              <span class="icon sun">&#9788;</span>
              <span class="icon moon">&#9789;</span>
            </span>
          </label>
        </div>
        <div class="toc-mobile">
          <button id="tocToggle" aria-expanded="false" aria-controls="tocList">☰ Inventory</button>
          <nav id="tocList" class="toc" aria-label="On this page"></nav>
        </div>
      </div>
    </div>
  </header>

  <div class="page">
    <aside class="toc-desktop">
      <nav class="toc" aria-label="On this page"></nav>
    </aside>
    <main id="content">

      <section>
        <h2>What this guide is for</h2>

            <p class="kicker">
              You’ve probably used ChatGPT, Claude, Gemini, or Copilot—and perhaps you even went a little crazy and tried Grok. You’ve seen AI do impressive things. You also may have seen it confidently state that there are two R's in strawberry.
            </p>
            <p>
              While this guide won’t make you an AI researcher, it will give you the information you need to:
            </p>
            <ul>
              <li>Write better prompts that get more predictable, verifiable results</li>
              <li>Understand why AI “forgets,” drifts, or hallucinates</li>
              <li>Run multi-step workflows that produce usable results</li>
            </ul>

            <h3>The AI restaurant sim</h3>
            <p>
              Throughout this guide, we'll be using a kitchen sim metaphor to help reinforce each concept:
            </p>
            <ul>
              <li>The <strong>Cook (AI model)</strong> works quickly, but can only create using the ingredients and tools in front of them.</li>
              <li>Your <strong>Order Ticket (prompt)</strong> is the objective, rules, and output format: what to cook, substitutions/allergies, and how to plate it.</li>
              <li>The <strong>Prep Counter (context)</strong> is everything the chef has right in front of them to work with: attachments or links submitted with the prompt.</li>
              <li>Your <strong>Inventory (context window)</strong> is limited. The counter space gets too crowded, older items might fall off.</li>
              <li><strong>Tools (calendar, spreadsheet, search)</strong> determine if the chef has access to a pantry for extra ingredients, cookbooks to research recipes, or use an appliance like a brulee torch.</li>
            </ul>
            <div class="image-container">
              <img src="../images/how-llms-work-banner.png" style="width:100%; height:auto;" loading="lazy" alt="Pixel-art illustration of an AI kitchen with a chef, prep counter, order tickets, and tools">
            </div>

            <div class="truth"><strong>Kitchen truth:</strong> Our chefs may be brilliant, but they have a limited attention span—if it’s not on the ticket or the counter, it doesn’t exist.</div>
      </section>

      <section>
        <h2>Pre-Shift Briefing: The 5-Minute LLM Kitchen Tour</h2>
          <p class="kicker">
            If you only read one part of this guide, read this section. Think of it as the pre-shift huddle before a busy dinner service: how the kitchen works, where things go wrong, and how to write order tickets that actually get you what you asked for.
          </p>

          <h3>How LLMs work in plain English</h3>
          <p>
            An LLM does not "know" facts the way a person does. It predicts the next word based on patterns from training plus whatever is currently on the prep counter (your context). It repeats that process one word at a time until it has a full response. That is why outputs can sound confident and coherent even when the model is wrong.
          </p>
          <p>
            Your prompt is the order ticket, and it controls far more than most people expect. A vague ticket gets generic output. A specific ticket with goal, audience, constraints, format, and source material gives the model rails to run on. Better prompts do not just improve style, they improve reliability.
          </p>
          <p>
            Context is limited, attention is finite, and models can drift as conversations get long. If critical details are not present and explicit in the latest context, quality drops. For important work, restate the objective, include the source, and ask for a structured answer that can be checked.
          </p>

          <h3>Kitchen Hazards: hallucinations and how to catch them</h3>
          <p>
            A hallucination is not the model "lying." It is usually the model completing a pattern when it lacks solid grounding. Treat outputs as drafts until verified, especially when names, numbers, dates, citations, policy, legal, medical, or financial claims are involved.
          </p>
          <ul>
            <li>Ask for uncertainty explicitly: "If you are unsure, say what is unknown."</li>
            <li>Require evidence: "Cite the source section or quote the provided text."</li>
            <li>Force checks: "List assumptions and a verification checklist before final answer."</li>
          </ul>

          <h3>Writing a Great Order Ticket: prompt building that works</h3>
          <p>
            Use this template for most business tasks:
          </p>
          <div class="code" data-label="Prompt blueprint">
            <div class="code-type" style="white-space:normal">Role: You are [role].</div>
            <div class="code-type" style="white-space:normal">Goal: Produce [specific outcome] for [audience].</div>
            <div class="code-type" style="white-space:normal">Context: Use only [documents/data pasted below].</div>
            <div class="code-type" style="white-space:normal">Constraints: [tone, scope, legal/compliance limits, do/don't].</div>
            <div class="code-type" style="white-space:normal">Output format: Return [exact structure: bullets/table/JSON/email draft].</div>
            <div class="code-type" style="white-space:normal">Quality bar: Include [assumptions, risks, confidence, next steps].</div>
          </div>
          <p>
            Then iterate once or twice instead of trying to get perfection in one shot. Ask the model to critique its own draft against your requirements, revise, and call out anything still uncertain. That single habit usually produces the biggest quality jump.
          </p>

          <div class="truth"><strong>Kitchen truth:</strong> Great output is rarely one lucky prompt. It is clear instructions, grounded context, and one or two disciplined revisions.</div>

      </section>

      <section>
        <h2>Mise en Place: How LLMs Generate Text (and Why the Prompt Matters)</h2>
          <p>
            Everyone around you has probably been raving about AI recently. Your social media feed is full of influencers showing how AI sends them a text every morning with a daily to do list, plans all their meals, and reads them a bedtime story at night. 
          </p>
          <div class="grid">
            <div>
              <p>
                So, you decide to try it out by asking for something simple, it answers confidently:
              </p>
            </div>
          </div>
          <div class="grid">
            <div>
              
              <div class="code" data-label="Prompt">
                Who would win in a fight? Elon Musk or Mike Tyson?
              </div>
              <div class="code" data-label="Grok:">
                Mike Tyson packs legendary knockout power that could end it quick, but Elon's relentless endurance from 100-hour weeks and adaptive mindset outlasts even prime fighters in prolonged scraps. Tyson's age tempers explosiveness, while Elon fights smarter—feinting with strategy until Tyson fatigues. Elon takes the win through grit and ingenuity, not just gloves.<span class="cursor animate">_</span>
              </div>
            </div>
            <div class="mini-widget">
              <p id="tokenParagraph">
                This isn't necessarily because that AI model is faulty or inferior to another model that gave you a better answer. It could be a result of a bad prompt or improper utilization of that particular model. Which is why it's important to understand how LLMs work and how they generate text.
              <br><br>
                It's best to think of an LLM as a chef or line cook at a restaurant. It doesn’t cook the whole dish in one step. It adds one ingredient (small chunk of text called a <strong>token</strong>) at a time. After each ingredient, it reviews what’s available on the <strong>prep counter (context)</strong> and the <strong>order ticket (prompt)</strong>, then asks: “Given what I can see right now, what’s the most plausible next ingredient?”
              </p>
              <button class="sandbox-btn" id="toggleTokens" type="button" aria-pressed="false" aria-controls="tokenParagraph">
                Highlight tokens
              </button>
            </div>
        </div>

        <div class="truth"><strong>Kitchen truth:</strong> Tokens aren't just how the model reads and generates text—they also determine pricing, latency, and maximum output length.</div>

        <h3>Your phone already does this (badly)</h3>
        <p>
          You've actually seen this mechanism before: start typing a text message and your phone suggests the next word. Type "I went to the" and it offers "store" | "park" | "hospital." That's next-word prediction—called an <strong>n-gram</strong>—the same core idea behind every LLM.
        </p>
        <p>
          The difference? Your phone's keyboard uses a simple lookup: "what word most commonly follows these last one or two words?"
          It doesn't know you were planning a beach trip three messages ago. It doesn't understand context—it understands <strong>frequency</strong>.
        </p>

        <div class="sandbox-card" id="predictGame">
          <label class="sandbox-label"><strong>Try it: guess the next word like an n-gram</strong></label>
          <p id="pgProgress" class="note" style="margin:0 0 4px"></p>
          <div class="code" data-label="Complete the sentence:">
            <div class="code-type" id="pgSentence" style="min-height:1.4em;white-space:normal"></div>
          </div>
          <div id="pgChoices"></div>
          <div id="pgReveal" class="callout" style="display:none" aria-live="polite"></div>
          <div style="display:flex; gap:8px; padding:4px 0">
            <button id="pgBack" class="sandbox-btn" type="button" style="display:none">Back</button>
            <button id="pgNext" class="sandbox-btn" type="button" style="display:none">Next round</button>
            <button id="pgReset" class="sandbox-btn" type="button" style="display:none">Start over</button>
          </div>
        </div>

        <h3>The upgrade: looking at the whole counter</h3>
        <p>
          LLMs use the same core mechanic—predict the next token—but instead of glancing at the last word or two, they look at everything currently on the counter (the full context window). That's why an LLM can connect "surgeon" to "scalpel" even when they're sentences apart.
        </p>

        <div class="truth"><strong>Kitchen truth:</strong> Your phone keyboard is a cook who only looks at the last ingredient added. An LLM is a cook who glances at the whole counter—but it's still cooking one ingredient at a time.</div>

        <details class="tip">
          <summary><strong>Power-up (optional): N-grams—the phone's cheat sheet</strong></summary>
          <p>
            An <strong>n-gram</strong> is a sequence of <em>n</em> consecutive words. Your phone's predictions are powered by tables of these:
          </p>
          <ul>
            <li><strong>Unigram (n = 1):</strong> single-word frequency. "The" is one of the most common English words, so it gets suggested constantly.</li>
            <li><strong>Bigram (n = 2):</strong> word pairs. After "of the" → "most" | "best" | "world." After "going to" → "be" | "the" | "get."</li>
            <li><strong>Trigram (n = 3):</strong> three-word chains. "One of the" → "most." "I want to" → "go" | "be" | "know."</li>
          </ul>
          <p>
            The phone stores a frequency table of these sequences (often personalized to your texting habits) and picks the highest-scoring match. The window is tiny—usually 1–3 words—so it has no ability to track meaning, themes, or instructions from earlier in the conversation.
          </p>
          <p>
            Try this: open your phone, type a word, then keep tapping the middle suggestion 20 times. You'll get a grammatically plausible but meaningless sentence—that's n-gram prediction running on autopilot with no broader understanding.
          </p>
        </details>

        <h3>Same prompt, different response</h3>
        <p>
          You may have noticed that entering the same prompt into the same LLM can produce different answers. This is because when the model generates text, it considers lots of possibilities for the next word (token) and scores them. Then the system <strong>chooses one</strong> based on a setting referred to as the <strong>temperature</strong> which controls how strongly it sticks to the top choice vs. how willing it is to take a less-likely option.
        </p>
        <p>
          Imagine ordering chocolate chip cookies for dessert—a temperature of 0 means the chef follows the standard recipe exactly as written. A temperature of 1 means the chef will put its own spin on the recipe, so you might get oatmeal chocolate chip or browned butter pecan, but you're still getting cookies. Now a temperature of 2 means the chef can get truly creative and you might end up receiving spaghetti and watermelon balls with a chocolate marinara, topped with a candied fennel garnish.
        </p>
        <div class="grid">
          <div class="callout">
            <h3 style="margin-top:0">Standard recipe</h3>
            <p style="margin:0">Stable and repeatable (nearly the same each run).</p>
          </div>
          <div class="callout">
            <h3 style="margin-top:0">Chef’s special</h3>
            <p style="margin:0">Varied and improvisational (poetic or pure nonsense).</p>
          </div>
        </div>

        <div class="truth"><strong>Kitchen truth:</strong> Most models you'll encounter have a set temperature, but some models do allow you to adjust it yourself.</div>

        <div class="sandbox-card" id="tempDemo">
          <label for="tempSlider" class="sandbox-label"><strong>Try it: drag the temperature slider to complete the sentence</strong></label>
          <div style="display:flex; align-items:center; gap:12px">
            <span class="note">Standard recipe</span>
            <input id="tempSlider" type="range" min="0" max="100" value="0" step="1"
              style="flex:1; min-width:120px" aria-label="Temperature slider from 0 to 2" />
            <span class="note">Chef's special</span>
          </div>
          <div style="text-align:center; margin:6px 0">
            <span class="badge" id="tempValue">Temperature: 0.0</span>
          </div>
          <div class="code" data-label="LLM output:">
            <div class="code-type" style="white-space:normal">The sky is<span id="tempOutput">...</span><span class="cursor animate">_</span></div>
          </div>
        </div>

        <h3>Why prompts matter more than people expect</h3>
        <p>
          The model’s default job is to produce a coherent continuation of whatever you put in front of it. If your prompt is vague, “coherent” often means it picks an interpretation it thinks is most likely and commits—hello, <strong>chocolate spaghetti with watermelon</strong>. If your prompt is specific (gluten-free vegan chocolate chip cookies with cinnamon, no fruit or nuts), “coherent” shifts towards what you actually want.
        </p>

        <details class="tip">
          <summary><strong>TL;DR</strong></summary>
          <ul>
            <li>Your order ticket (prompt) tells the chef (LLM model) what you want.</li>
            <li>The chef then cooks your order one ingredient (token) at a time.</li> 
            <li>A more specific order ticket doesn't make the chef better at cooking—it provides the chef more context to make your order the way you envisioned.</li>
            <li>Different chefs can cook the same order in vastly different ways. If you want the standard recipe, Copilot might be your best bet; but if you're feeling adventurous and have a steel gut, Grok is ready with the Chef's special.</li>
          </ul>
        </details>
      </section>
      <section>
        <h2>The Prep Counter: Context Windows, Drift, “Forgetting,” and the Dreaded Hallucination</h2>

        <p>
          In the last section we established that our LLM chef generates text one token at a time by scanning everything on its prep counter. But here's the catch: <strong>that counter is not infinite</strong>. Every model has a fixed amount of counter space called its <strong>context window</strong>, measured in tokens. Once the counter is full, something has to go.
        </p>

        <h3>Counter space: bigger isn't always better</h3>
        <p>
          Context windows have gotten dramatically larger over the past few years. Early GPT models had roughly 4,000 tokens of counter space—about three pages of text. Today, some models advertise windows of 128K, 200K, or even 1 million tokens. That sounds like the chef upgraded from a cutting board to a full commercial kitchen island. And to help our chef work quickly and efficiently, all that additional space came with new cookbooks and countertop appliances that also use that new space.
        </p>
        <p>
          But here's what the marketing doesn't tell you: <strong>more counter space also means more stuff for our chef to keep track of</strong>. So our chef might remember what was on the counter when they started cooking and they can easily see the last few items that were added, but everything in between is slowly being buried or shoved to the side. This is where <strong>attention</strong> comes into play.
        </p>

        <h3>Attention—how LLMs look at the whole counter</h3>
        <div class="grid">
          <div>
            <p>
              In n-gram prediction, the model only sees a fixed window of 1–3 words. LLMs replace that tiny window with a mechanism called <strong>attention</strong>. For each new token the model is about to generate, attention assigns a <strong>relevance score</strong> between that position and every other token already on the counter. Think of it as the cook glancing back across all the ingredients and deciding which ones matter <em>right now</em>. This is the core of the <strong>transformer</strong> architecture (the "T" in GPT). There is a tradeoff though: attention across all tokens is computationally expensive and scales with the size of the context—which is one reason context windows have limits and longer prompts cost more.
            </p>
          </div>
          <!--Image Comparison Slider-->
          <div>
            <figure dir="rtl">
              <p class="image-compare" role="img" tabindex="0" aria-label="Interactive comparison of LLM context windows with and without attention highlighting." aria-description="Two overlaid images are revealed by a slider. One shows an LLM context window with attention weights highlighted, illustrating how the model focuses on relevant tokens. The other shows the same context window without highlighting, representing raw token storage.">
                <span>
                  <img src="../images/LLM-context-window-attn.png" alt="" aria-hidden="true" loading="lazy">
                  </span>
                  <img src="../images/LLM-context-window.png" alt="" aria-hidden="true" loading="lazy">
              </p>
            </figure>
          </div>
        </div>
        <details class="tip">
          <summary><strong>Power-up (optional): The mechanics of attention</strong></summary>
          <p>
            <strong>Attention</strong> is the Transformer mechanism that lets each token <strong>pull in and mix information from other tokens</strong> in the context. It acts like differentiable retrieval: “given what I need at this position, which earlier tokens are most relevant?”
          </p>
        
          <p>
            Mechanically, the model projects token representations <code>X</code> into <strong>queries, keys, values</strong>:
          </p>
        
          <ul>
            <li><code>Q</code>: what this token is looking for</li>
            <li><code>K</code>: how each token can be matched</li>
            <li><code>V</code>: the information to combine</li>
          </ul>
        
          <p>
            It computes relevance with <strong>scaled dot products</strong>, normalizes with <strong>softmax</strong>, then mixes values:
          </p>
        
          <pre><code>Attn(Q, K, V) = softmax((QKᵀ / √d_k) + mask) V</code></pre>
        
          <p>
            In GPT-style LLMs, a <strong>causal mask</strong> blocks attention to future tokens so generation is valid.
          </p>
        
          <p>
            LLMs use <strong>multi-head attention</strong> so different heads can learn different dependency patterns in parallel, and they inject <strong>positional information</strong> (e.g., RoPE/relative encodings) so attention is sensitive to word order. During generation, <strong>KV caching</strong> stores past keys/values to avoid recomputing them each step. Full attention scales <strong>quadratically</strong> with context length (<code>O(n^2)</code>), which is why long contexts are expensive.
          </p>
        </details>

        <h3>Drift: when the model gradually stops following the rules</h3>
        <p>
          In long AI conversations, you may see the model follow your instructions perfectly at first—then slowly begin to bend them. For example: your prompt says “use bullet points only,” it complies, and ten messages later it starts writing paragraphs. That gradual loss of adherence is known as <strong>drift</strong>.
        </p>
        <p>
          Drift happens because the model is generating each reply based on the entire visible conversation. As the thread grows, your original instructions become a smaller and less prominent part of the context compared to newer messages, new subgoals, and recent wording. The model also tends to match the most recent style and priorities unless constraints are kept explicit.
        </p>
        <p>
          To reduce drift, periodically <strong>restate the constraints that matter</strong>, preferably as a short, consistent rule block (e.g., “Format: bullets only. Length: &lt;150 words. Tone: direct.”). This keeps the rules salient and reduces the chance they’re implicitly overridden by newer content.
        </p>

        <h3>“Forgetting”: the counter has an edge</h3>
        <p>
          Drift is gradual—your instructions are still technically on the counter, just buried. <strong>Forgetting</strong> is what happens when the conversation exceeds the context window entirely. At that point, the oldest messages literally fall off the edge of the counter. The chef doesn't know they ever existed.
        </p>
        <p>
          This is not a bug. It's a hard physical constraint called <strong>context overflow</strong>—like a counter that can hold a maximum of 10 ingredients. Adding an 11th ingredient knocks ingredient one to the floor. Most LLM interfaces handle this silently: they drop or summarize the oldest messages to make room, and the model has no idea anything was removed.
        </p>
        <p>
          This is why you might have a conversation where the AI researched a song title for you in message three, and by message 40 it has no memory of that song existing. It simply slid off the counter.
        </p>

        <div class="grid">
          <div class="callout">
            <h3 style="margin-top:0">Drift</h3>
            <p style="margin:0">Your instructions are still on the counter but buried under newer context. The chef <em>could</em> see them—it's just focused elsewhere.</p>
          </div>
          <div class="callout">
            <h3 style="margin-top:0">Forgetting</h3>
            <p style="margin:0">Your instructions have fallen off the counter entirely. The chef <em>cannot</em> see them—they no longer exist in its world.</p>
          </div>
        </div>

        <div class="sandbox-card" id="overflowDemo">
          <label class="sandbox-label"><strong>Try it: fill the prep counter until something falls off</strong></label>
          <p id="ofStatus" class="note" style="margin:0 0 4px"></p>
          <div class="kitchen-scene" id="ofScene"></div>
          <div style="display:flex; gap:8px">
            <button id="ofAdd" class="sandbox-btn" type="button">Send message</button>
            <button id="ofResend" class="sandbox-btn" type="button" style="display:none">Resend prompt</button>
            <button id="ofReset" class="sandbox-btn" type="button">Reset</button>
          </div>
          <div id="ofReveal" class="callout" style="display:none" aria-live="polite"></div>
        </div>

        <div class="truth"><strong>Kitchen truth:</strong> Many LLMs allow you to group chats into projects. This allows you to specify rules that apply to everything inside that project which can help prevent drift and forgetting.</div>

        <details class="tip">
          <summary><strong>Power-up (optional): What actually happens when context overflows</strong></summary>
          <p>
            Different platforms handle overflow differently, and most don't tell you when it's happening:
          </p>
          <ul>
            <li><strong>Truncation:</strong> The oldest messages are silently dropped. This is the simplest approach—first in, first out.</li>
            <li><strong>Sliding window:</strong> The system keeps your original system prompt pinned at the top and drops the oldest conversation turns from the middle. This preserves your initial instructions but loses early conversation context.</li>
            <li><strong>Summarization:</strong> Some systems use a secondary model to compress older messages into a short summary before dropping them. The chef gets a sticky note that says “earlier we discussed X” instead of the full conversation—better than nothing, but details get lost.</li>
          </ul>
          <p>
            None of these are perfect. The practical takeaway: for important, multi-step work, don't rely on a single marathon conversation. Break it into shorter sessions and carry forward the key context yourself.
          </p>
        </details>

        <h3>Hallucination: the chef that won't say “I don’t know”</h3>
        <p>
          This is the big one—the behavior that makes people distrust AI entirely, or worse, trust it when they shouldn’t.
        </p>
        <p>
          A <strong>hallucination</strong> is when a model produces a claim that sounds confident and well-formed, but isn’t actually supported by the information it has. It might invent a study, fabricate a quote, misstate a law, or confidently “fill in” a missing detail that was never verified.
        </p>
        <p>
          Under the hood, the core engine is still a next-token generator: given the text in front of it, it predicts what a good-sounding continuation looks like. If it’s operating in a <em>closed-book</em> mode (no tools, no retrieval), it isn’t checking a database or consulting the internet—it’s pattern-completing from what it learned during training and what you’ve provided in the chat. When the real answer isn’t present, the model can generate something that <em>resembles</em> an answer without actually being one.
        </p>
        <p>
          Modern systems complicate this in a good way: many LLM products can now <strong>look things up</strong> using search, retrieval (RAG), or other tools, and that can dramatically reduce hallucinations—<em>when the tool is actually used</em> and the retrieved sources are reliable. But it doesn’t make the problem disappear. The model can still misread what it finds, summarize incorrectly, mix together multiple sources, cite the wrong thing, or—if lookup fails or isn’t available—quietly fall back to fluent guessing.
        </p>
        <p>
          Back in the kitchen: sometimes the chef can step off the line and check the cookbook, call the supplier, or peek in the walk-in—that’s browsing and
          retrieval. It helps. But what if the cookbook is missing a page, the supplier gives bad info, or the chef is rushing? And if the chef has been trained to always serve <em>something</em>, they’ll improvise rather than admit they don't know how to make your order. So, you still get a plate that <em>looks</em> right but it might not be edible.
        </p>

        <details class="tip">
          <summary><strong>Power-up (optional): Why can't they just… not hallucinate?</strong></summary>
          <p>
            This is one of the most common questions people ask. Part of the answer is architectural, and part of it is incentives.
          </p>   
          <p>
            At the core, an LLM generates text by predicting likely next tokens from patterns learned during training. It isn’t running a “truth check” as it writes, and it doesn’t automatically ground claims in a source of record. So it can produce a fluent sentence that <em>sounds</em> right even when it isn’t.
          </p>     
          <p>
            It’s also worth separating <strong>uncertainty</strong> from <strong>honesty</strong>. The model may be internally “unsure” in the sense that multiple continuations are plausible, but unless the system is explicitly designed to surface that uncertainty, it still has to pick one concrete continuation—and concrete answers often come out sounding confident by default.
          </p>      
          <p>
            On top of that, many chat models are <strong>tuned to be helpful and decisive</strong>. Depending on how they’re trained and deployed, they may be implicitly (or explicitly) discouraged from responding with “I don’t know.” If the system rewards always producing an answer—or penalizes refusals and hedging—the model learns a style that fills gaps with plausible detail instead of stopping short.
          </p>
          <p>
            Researchers mitigate hallucination in layers: retrieval-augmented generation (RAG) can ground responses in provided sources; preference tuning (often called RLHF) can teach models to hedge, refuse, or ask for clarification; and some products add post-generation verification (citations, consistency checks, tool-based fact checking). These help a lot, but they don’t fully eliminate hallucination—because the underlying generator is still optimizing for “plausible continuation,” not “guaranteed truth.”
          </p>
        </details>

        <h3>Why hallucinations are hard to spot</h3>
        <p>
          The dangerous part isn't that LLMs make things up—it's that the made-up content is <strong>structurally indistinguishable</strong> from the accurate content. The model uses the same confident tone, the same formatting, the same grammatical polish whether it's giving you a real fact or an invented one. There's no italics for “I'm guessing here” and no red flag emoji for “I made this up.”
        </p>
        <p>
          This means verification is not optional. It's not a nice-to-have for cautious people—it's a core part of using these tools responsibly. If the model gives you a statistic, a name, a date, a quote, a URL, or a citation: <strong>check it</strong>. Not because the model is bad, but because it was never designed to be a fact database. It was designed to produce plausible text.
        </p>

        <div class="grid">
          <div class="callout">
            <h3 style="margin-top:0">Likely reliable</h3>
            <ul style="margin:0">
              <li>Well-known facts</li>
              <li>General explanations of common concepts</li>
              <li>Code patterns for popular frameworks</li>
              <li>Structural tasks (formatting, summarization, rewriting)</li>
            </ul>
          </div>
          <div class="callout">
            <h3 style="margin-top:0">Verify before trusting</h3>
            <ul style="margin:0">
              <li>Specific statistics, dates, or numbers</li>
              <li>Direct quotes attributed to real people</li>
              <li>Citations, URLs, or references to studies</li>
              <li>Niche or recent facts</li>
            </ul>
          </div>
        </div>

        <div class="sandbox-card" id="hallucinationQuiz">
          <label class="sandbox-label"><strong>Try it: spot the hallucination</strong></label>
          <p id="hqProgress" class="note" style="margin:0 0 4px"></p>
          <div class="code" data-label="AI-generated claim:">
            <div class="code-type" id="hqClaim" style="min-height:1.4em;white-space:normal"></div>
          </div>
          <div id="hqChoices">
            <button id="hqFact" class="sandbox-btn" type="button">Fact</button>
            <button id="hqMyth" class="sandbox-btn" type="button">Myth</button>
          </div>
          <div id="hqReveal" class="callout" style="display:none" aria-live="polite"></div>
          <div style="display:flex; gap:8px; padding:4px 0">
            <button id="hqBack" class="sandbox-btn" type="button" style="display:none">Back</button>
            <button id="hqNext" class="sandbox-btn" type="button" style="display:none">Next claim</button>
            <button id="hqReset" class="sandbox-btn" type="button" style="display:none">Start over</button>
          </div>
        </div>

        <details class="tip">
          <summary><strong>TL;DR</strong></summary>
          <ul>
            <li>The <strong>context window</strong> (prep counter) is finite and every model has a hard limit.</li>
            <li><strong>Attention</strong> is how the LLM decides what is most relevant when looking at everything in the context window.</li>
            <li><strong>Drift</strong> happens when your original instructions get drowned out by a growing conversation. Restate key rules periodically.</li>
            <li><strong>Forgetting</strong> happens when conversation history exceeds the context window and older messages are silently dropped. Break long tasks into shorter sessions.</li>
            <li><strong>Hallucinations</strong> happen because the model predicts plausible text, not verified facts. Models are trained to be helpful, so they default to producing an answer—even when they probably shouldn't.</li>
            <li>Verification is not optional. If the output includes specific facts, check them before trusting them.</li>
          </ul>
        </details>
      </section>

      <section>
        <h2>The Order Ticket: Why One Prompt Is Never Enough</h2>

        <p>
          You now understand how the chef cooks (one token at a time) and how the prep counter works (finite, prone to drift and forgetting). The third piece of the puzzle—and the one you have the most control over—is the <strong>order ticket</strong> itself: your prompt.
        </p>
        <p>
          Most guides on prompt engineering focus on clever tricks: magic phrases, special formatting, secret keywords that unlock better output. Some of those tips are useful. But the single most important skill isn't writing one perfect prompt—it's understanding that <strong>one prompt is almost never enough</strong>.
        </p>

        <h3>"Make me something good"</h3>
        <p>
          Walk into a restaurant and tell the chef "make me something good." You'll get a dish. It might even be decent. But it probably won't be what you had in mind, because the chef had to guess your preferences, dietary restrictions, mood, and expectations—all from four words.
        </p>
        <p>
          That's what most people do with AI. They type a single prompt, get a mediocre result, and conclude the tool isn't very useful. But the problem wasn't the chef—it was the ticket. A vague order produces a generic dish. Every time.
        </p>

        <div class="truth"><strong>Kitchen truth:</strong> "Make me something good" will always get you the chef's default. If you don't specify, the chef fills in every blank with whatever seems most likely—and "most likely" is rarely the same as "what you actually wanted."</div>

        <p>Now, "make me something good" isn't necessarily a bad prompt—in fact, open-ended prompts can be useful early on, when you’re exploring and need inspiration or a brainstorming partner. Once you do have a clear sense of what you want (and what you don’t), it’s often worth starting a new chat with a fresh prompt that states those requirements explicitly. This keeps outdated assumptions and abandoned ideas out of the context window—and makes it easier for the model to stay aligned as the conversation grows.</p>

        <h3>One ticket is never enough</h3>
        <p>
          Here's what experienced AI users understand that casual users don't: <strong>the first response is just a draft, not a finished product</strong>. The real work starts after you read what comes back.
        </p>
        <p>
          Think about how you'd approach the same task without AI. If you needed to write a project proposal, you wouldn't sit down and produce a polished final version in one pass. You'd outline, draft, re-read, cut what doesn't work, restructure, get feedback, and revise. The process is iterative because good work is iterative.
        </p>
        <p>
          AI is the same. The best results come from a <strong>draft-and-revise loop</strong>:
        </p>
        <ol>
          <li><strong>First pass:</strong> Give the model a prompt. Read what comes back. Don't accept or reject—<em>evaluate</em>.</li>
          <li><strong>Identify gaps:</strong> What's wrong? What's missing? What did it misinterpret? What's actually good that you want to keep?</li>
          <li><strong>Refine:</strong> Update your prompt with more specific instructions, corrections, or constraints. Run it again.</li>
          <li><strong>Repeat</strong> until the output matches what you need—or until you realize you need to restructure your approach entirely.</li>
        </ol>
        <p>
          This is not a sign of failure. A chef expects to taste and adjust seasoning. A designer expects to iterate on mockups. A writer expects to revise drafts. The tool didn't fail—you're just not done yet.
        </p>

        <div class="sandbox-card" id="iterationDemo">
          <label class="sandbox-label"><strong>Try it: watch a prompt improve through revision</strong></label>
          <p id="itProgress" class="note" style="margin:0 0 4px"></p>
          <div class="code" id="itPromptBlock" data-label="Order ticket (draft 1):">
            <div class="code-type" id="itPrompt" style="white-space:normal"></div>
          </div>
          <div class="code" data-label="Chef produces:">
            <div class="code-type" id="itOutput" style="white-space:normal"></div>
          </div>
          <div id="itNotice" class="callout" aria-live="polite"></div>
          <div style="display:flex; gap:8px; padding:4px 0">
            <button id="itBack" class="sandbox-btn" type="button" style="display:none">Back</button>
            <button id="itNext" class="sandbox-btn" type="button">Next revision</button>
            <button id="itReset" class="sandbox-btn" type="button" style="display:none">Start over</button>
          </div>
        </div>

        <h3>Why AI can't replace experience</h3>
        <p>
          There's a persistent fantasy that AI will let anyone produce expert-level work without expert-level knowledge. Write legal contracts without understanding law. Debug code without knowing how to program. Design systems without production experience. Just type what you want and let the AI handle the rest.
        </p>
        <p>
          This gets the relationship exactly backwards. AI is an <strong>amplifier</strong>, not a replacement. It amplifies what the user already knows. An experienced developer using AI can produce code faster because they can read the output, spot the bugs, evaluate the architecture, and give precise feedback when something is wrong. A non-developer using the same AI will accept buggy code, miss security vulnerabilities, and have no way to evaluate whether the output actually works—because they don't know what to look for.
        </p>
        <p>
          Back in the kitchen: an experienced foodie can taste a dish and say "needs acid, too much salt, the protein is overcooked." That feedback is specific and the chef can adjust precisely. Someone who's never cooked might say "it tastes weird" or, worse, not notice anything is wrong at all.
        </p>

        <div class="truth"><strong>Kitchen truth:</strong> AI amplifies your vision—it doesn't replace it. If you can't tell a good dish from a bad one, a chef can't help you.</div>

        <h3>What goes into a good prompt</h3>
        <p>
          There's no single "correct" prompt format, but effective prompts tend to include the same kinds of information:
        </p>
        <ul>
          <li><strong>Role / context:</strong> Who is this for? "You're writing for a technical audience" or "explain like I'm a 5-year-old" gives the model a frame to work within.</li>
          <li><strong>Task:</strong> What specifically needs to be done? Not "help me with my proposal" but "write a 300-word introduction arguing that AI-powered rocks are the way of the future."</li>
          <li><strong>Constraints:</strong> What should the model avoid or limit? "No jargon," "under 500 words," "don't include personal opinions."</li>
          <li><strong>Format:</strong> How should the output be structured? "Use bullet points," "respond as a table with columns for X, Y, and Z."</li>
          <li><strong>Examples:</strong> If you have a specific style or pattern in mind, show it. Models are excellent at pattern-matching from examples.</li>
        </ul>
        <p>
          You don't need all five for every prompt. A quick question doesn't need role and format specifications. But for anything substantial—drafting content, analyzing data, generating code—the more of these fields you fill in, the closer the first draft will be to what you actually need, and the fewer revision cycles it will take.
        </p>

        <div class="grid">
          <div class="callout">
            <h3 style="margin-top:0">Vague prompt</h3>
            <p style="margin:0">"Write me an email about the project update."</p>
          </div>
          <div class="callout">
            <h3 style="margin-top:0">Specific prompt</h3>
            <p style="margin:0">"Write a 3-paragraph email to my team summarizing this week's iRock Pro MAX+ progress. Tone: professional but warm. Include: API migration is 80% complete, QA starts Monday, need 3 volunteers for user testing by EOD Friday. End with bullet-point action items."</p>
          </div>
        </div>

        <div class="truth"><strong>Kitchen truth:</strong> A specific ticket doesn't make the chef more talented—it just stops the chef from guessing. Every detail you provide is one fewer gap the model fills with its own assumptions.</div>

        <details class="tip">
          <summary><strong>TL;DR</strong></summary>
          <ul>
            <li>A single prompt rarely produces a finished result. Treat the first response as a rough draft and iterate.</li>
            <li>The revision loop—prompt, evaluate, refine, repeat—is the normal workflow, not a sign of failure.</li>
            <li>AI amplifies expertise. Professionals get better results because they can evaluate and correct the output. Novices can't tell when the output is wrong.</li>
            <li>Good prompts include context, a specific task, constraints, output format, and examples when relevant.</li>
            <li>You don't need prompt tricks. You need the same skills you'd use doing the work manually: clear thinking, critical evaluation, and willingness to revise.</li>
          </ul>
        </details>
      </section>

      <section>
        <h2>Running the Line: Workflows, Agents, and the Expo Pass</h2>

        <p>
          You can now write a solid order ticket and iterate until the dish is right. But some orders are too big for a single ticket—no matter how refined it is. Asking one prompt to research, analyze, draft, format, and fact-check a complex deliverable is like asking one cook to simultaneously run the grill, the sauté station, and the dessert counter. It might work for a grilled cheese. It won't work for a five-course meal.
        </p>
        <p>
          This is where <strong>station workflows</strong> come in: breaking a complex task into a sequence of focused steps, each with its own prompt, its own inputs, and a clear handoff to the next.
        </p>

        <h3>Why single prompts break down</h3>
        <p>
          For simple tasks—a quick summary, a format conversion, a short email—a single prompt works fine. But as complexity grows, three problems compound:
        </p>
        <ul>
          <li><strong>Context overload:</strong> The more you ask for in one prompt, the more the model has to juggle. Instructions get buried, priorities blur, and the output becomes a mediocre average of everything you asked for instead of being excellent at any one thing.</li>
          <li><strong>No checkpoints:</strong> If step 2 goes wrong in a 5-step process and you only see the final output, you can't tell where things derailed. You receive a bad dish and no way to diagnose where the kitchen went wrong.</li>
          <li><strong>Drift at scale:</strong> Long, complex prompts fill the context window faster. The model starts losing track of constraints from earlier in the prompt—the same drift problem discussed earlier, compressed into a single turn.</li>
        </ul>

        <div class="truth"><strong>Kitchen truth:</strong> A single cook juggling every station simultaneously doesn't produce excellent dishes—if you're lucky, they'll produce mediocre ones. Complex tasks need a line of cooks, not a single hero.</div>

        <h3>The kitchen line</h3>
        <p>
          Professional kitchens don't have one person doing everything. They run a <strong>line</strong>: a sequence of specialized stations, each responsible for one part of the meal. Prep handles ingredients. Grill handles proteins. Sauté handles sides. Pastry handles dessert. The <strong>expediter</strong> (expo) sits at the end, inspecting every plate before it goes out.
        </p>
        <p>
          The same model works for AI. Instead of one massive prompt, you break the task into stations:
        </p>
        <ol>
          <li><strong>Each station has one job.</strong> "Clean up these notes." "Extract the action items." "Draft the summary." Not all three at once.</li>
          <li><strong>Each station gets focused input.</strong> The output of the previous station becomes the input for the next—not the entire conversation history.</li>
          <li><strong>You check at each handoff.</strong> Before passing output to the next station, review it. Catch errors now, not three stations later.</li>
        </ol>
        <p>
          This works whether you're running each station manually (copying output into a new prompt) or using tools that chain steps automatically. The principle is the same: <strong>focused steps with verified handoffs</strong>.
        </p>

        <div class="sandbox-card" id="workflowDemo">
          <label class="sandbox-label"><strong>Try it: step through a station workflow</strong></label>
          <p class="note" style="margin:0 0 4px">Task: turn rough meeting notes into a stakeholder-ready summary</p>
          <p id="wfProgress" class="note" style="margin:0 0 4px"></p>
          <div class="code" id="wfPromptBlock" data-label="Station 1—prompt:">
            <div class="code-type" id="wfPrompt" style="white-space:normal"></div>
          </div>
          <div class="code" data-label="Station output:">
            <div class="code-type" id="wfOutput" style="white-space:normal"></div>
          </div>
          <div id="wfNotice" class="callout" aria-live="polite"></div>
          <div style="display:flex; gap:8px; padding:4px 0">
            <button id="wfBack" class="sandbox-btn" type="button" style="display:none">Back</button>
            <button id="wfNext" class="sandbox-btn" type="button">Next station</button>
            <button id="wfReset" class="sandbox-btn" type="button" style="display:none">Start over</button>
          </div>
        </div>

        <div class="truth"><strong>Kitchen truth:</strong> The line works because each station has one job and one handoff. The moment you ask one station to do too many things, you've lost the advantage of having a line.</div>

        <h3>The expo pass: nothing leaves without a check</h3>
        <p>
          In a professional kitchen, no plate reaches the table without passing the <strong>expo</strong>—the expediter who checks every dish for accuracy, presentation, and completeness. Wrong garnish? Sent back. Missing side? Caught before the customer sees it.
        </p>
        <p>
          In AI workflows, the expo is <strong>you</strong>. After the final station produces output, you verify it before anyone else sees it:
        </p>
        <ul>
          <li><strong>Does it match the original request?</strong> Not what the model decided you meant—what you actually asked for.</li>
          <li><strong>Are the facts checkable?</strong> Any specific claims, statistics, or references should be verifiable from real sources.</li>
          <li><strong>Did anything get lost between stations?</strong> Compare the final output against earlier station outputs. Did the summary drop an action item? Did formatting swallow a data point?</li>
          <li><strong>Is the quality of the work sufficient?</strong> If not, it's not done.</li>
        </ul>
        <p>
          The expo pass isn't an optional step for cautious people. It's the step that makes everything before it worth doing. A kitchen that sends out unchecked plates will eventually send out something inedible—and an AI workflow that skips verification will eventually produce something inaccurate, embarrassing, or worse.
        </p>

        <h3>Agents: when the kitchen runs itself (mostly)</h3>
        <p>
          You may have heard the term <strong>AI agent</strong>—a system where the model doesn't just respond to a single prompt but actively plans, uses tools, and executes multiple steps on its own. Think of it as upgrading from a single cook who waits for orders to a small crew that can coordinate stations without you calling out every ticket.
        </p>
        <p>
          Agent capabilities are expanding rapidly: coding assistants that read files, write code, and run tests in sequence. Research tools that search the web, synthesize sources, and draft reports. Workflow tools that create project plans, draft communications, and schedule follow-ups—all from a single high-level instruction.
        </p>
        <p>
          This is powerful. It's also not magic. An agent is still running the same station-by-station workflow—it just handles the handoffs automatically instead of requiring you to copy-paste between prompts. The fundamental limits haven't changed:
        </p>
        <ul>
          <li>The context window is still finite. Agents manage it more efficiently, but they don't eliminate it.</li>
          <li>Hallucinations still happen. Agents with tool access hallucinate less, but not zero.</li>
          <li>Drift still compounds. A 15-step agent workflow can drift just like a 15-message conversation.</li>
          <li>Experience is still mandatory. Agents are fallible, so experience is needed for tighter prompts and reliable verification.</li>
        </ul>
        <p>
          The difference between using an agent well and using one poorly is the same as everything else in this guide: <strong>you're still the expediter</strong>. You set the objectives. You define the constraints. You check the output before it ships. The agent runs the line—you run the kitchen.
        </p>

        <details class="tip">
          <summary><strong>Power-up (optional): Tools—the pantry, the cookbook, and the torch</strong></summary>
          <p>
            Throughout this guide we've mentioned that some models can "look things up" or "use tools." Here's what that actually means under the hood.
          </p>
          <p>
            A base LLM can only work with what's already on the prep counter—your prompt, the conversation history, and whatever fits in the context window. <strong>Tools</strong> give the chef access to things beyond the counter: a pantry to grab fresh ingredients (file uploads, databases), a cookbook to look up recipes (web search, retrieval), or a specialized appliance like a brûlée torch (code execution, image generation, API calls).
          </p>
          <p>
            Mechanically, tool use works like this:
          </p>
          <ol>
            <li>The model decides it needs something it doesn't have and generates a structured request—essentially saying "I need to search for X" or "run this code."</li>
            <li>The system (not the model) executes that request: runs the search, executes the code, reads the file.</li>
            <li>The result gets placed back on the prep counter as new context, and the model continues generating with that information available.</li>
          </ol>
          <p>
            Common tool categories:
          </p>
          <ul>
            <li><strong>Search / browsing:</strong> The chef checks a cookbook or calls a supplier. Grounds responses in current, external information instead of relying on training memory alone.</li>
            <li><strong>Code execution:</strong> The chef fires up an appliance—running Python, doing math, processing data. The model writes the code; a sandbox executes it and returns results.</li>
            <li><strong>File reading:</strong> You hand the chef a document, spreadsheet, or image. The contents get placed on the counter as context for the model to work with.</li>
            <li><strong>APIs and plugins:</strong> The chef can call external services—check a calendar, query a database, send a message. Each integration extends what the kitchen can do without the chef needing to know how to build those systems.</li>
            <li><strong>Image generation:</strong> A separate specialist (a different model) that the chef can hand off to—"make this look like X." The chef writes the brief; the specialist produces the visual.</li>
          </ul>
          <p>
            The critical thing to understand: <strong>every tool result lands on the counter and takes up space</strong>. A web search that returns ten paragraphs of results eats the same context window as ten paragraphs of conversation. Tools expand what the chef can <em>access</em>, but they don't expand the counter itself.
          </p>
          <p>
            Tools also don't eliminate hallucinations—they reduce them. The chef can now check the cookbook, but can still misread the page, summarize it incorrectly, or mix up two recipes. Tool access makes the chef more capable, not infallible.
          </p>
        </details>

        <details class="tip">
          <summary><strong>TL;DR</strong></summary>
          <ul>
            <li>Complex tasks break down in a single prompt. Break them into focused stations, each with one job and clear input/output.</li>
            <li>The output of each station becomes the input for the next. Check quality at every handoff—don't blindly pass output forward.</li>
            <li>The <strong>expo pass</strong> is the final verification step. Nothing should leave your workflow without being checked against the original request.</li>
            <li><strong>Agents</strong> automate multi-step workflows but the same limits apply: finite context, possible hallucinations, potential drift.</li>
            <li>Whether you're running stations manually or delegating to an agent, you're still the expediter. Set objectives, define constraints, verify output.</li>
          </ul>
        </details>
      </section>

      <section>
        <h2>Bonus Quest: The Making of This Guide—AI Workflows in Action</h2>

        <h3>Building a "Choose Your Chef" character select screen</h3>
        <p>
          I've spent the last few years learning everything I can about how AI works because I always felt that the more I understand a tool, the better I can use it. That said, I still wouldn’t normally use AI for creating a guide like this one—I genuinely enjoy doing this sort of work myself—but I wanted to run an experiment: involve AI as much as possible at each stage of the project and see what it actually saved me. I also gave myself a slightly arbitrary constraint: <strong>three weeks</strong> to plan, design, build, and test the whole thing. Similar guides have taken me more than six weeks, so I was aiming for roughly a 50% reduction in timeline. My early research suggested that goal was optimistic at best… but I do love a challenge.
        </p>
        <p>
          A full behind-the-scenes would be longer than the guide itself, so this section focuses on <strong>one</strong> piece: a <strong>"Choose Your Chef"</strong> interactive—built like a fighting-game character select screen where you pick a model and see a sprite, a bio, and character stats.
        </p>

        <h3>Phase 1: Define requirements before writing code</h3>
        <p>
          I used AI early in the process for what I've found it most effective at in a new workflow: <strong>organizing, planning, and reviewing</strong>.
        </p>
        <p>
          Here's my initial prompt:
        </p>
        <div class="code" style="white-space:normal;" data-label="Initial planning prompt">
          I'm about to build a new "Sandbox" section in @guides/how-llms-work.html, and I'll be coding it myself (HTML/CSS/JS, no framework or build step).<br><br>

          The feature is a "Choose Your Chef" interactive in the style of a fighting-game character select screen,<br>
          using images in @images/character_select/ (note: images will be added later).<br><br>

          UI behavior:<br>
          &emsp;- The grid shows character selection boxes. For now each box uses a placeholder headshot.<br>
          &emsp;- Selecting a character shows the full sprite/illustration to the side (or below on mobile), plus:<br>
          &emsp;&emsp;- a short bio<br>
          &emsp;&emsp;- a standardized stats panel shared by all models, using 1–5 stars per stat<br>
          &emsp;- Use placeholders anywhere assets/content aren't ready yet.<br><br>

          What I need from you:<br>
          Help me plan the component and identify anything missing (data model, interactions, responsive layout decisions, accessibility, edge cases, asset naming conventions, etc.) before I start implementing.<br><br>

          Before we begin: what requirements, assumptions, or missing pieces should I lock down so this can be built cleanly?<br>
        </div>
        <p>
          Note that I specified that the goal wasn't to "generate code," but to <strong>identify gaps and create a checklist</strong>.
        </p>

        <h3>Phase 2: Build the wireframe</h3>
        <p>
          After I had the plan, I implemented the component myself.
        </p>
        <p>
          Not because AI can't generate code, but because experience has shown me that I can get a fully working prototype faster if I do it myself. Due to the time constraint and not having fully tested prompts for this particular type of component, doing it myself offered two advantages:
        </p>
        <ul>
          <li><strong>I can iterate faster</strong> when I understand every line, so I don't waste time hunting for the right code to modify.</li>
          <li><strong>Debugging is faster</strong> when the architecture matches how I think and I can quickly understand what's gone wrong and how to fix it.</li>
        </ul>
        <p>
          My loop looked like this:
        </p>
        <ol>
          <li><strong>Build the wireframe + interactions manually</strong> (grid selection → detail panel)</li>
          <li>Use AI to <strong>review</strong>: accessibility, keyboard states, responsive layout, edge cases</li>
          <li>Fix issues, tighten logic, and iterate</li>
        </ol>
        <p>
          <strong>Time saved:</strong> several hours. Having requirements and a checklist up front prevented trial and error loops.        
        </p>
        <div class="sandbox-card wireframe-demo" id="tplChefSelect">
          <label class="sandbox-label"><p>Here's the wireframe with placeholder content. The layout, selection behavior, and responsive grid were all functional before any content was created:</p></label>
          <div class="select-screen">
            <div class="portrait-row" id="tplRoster"></div>
            <div class="chef-panel" id="tplPanel">
              <div class="chef-art" id="tplArt"></div>
              <div class="chef-info" id="tplInfo"></div>
            </div>
          </div>
        </div>

        <h3>Phase 3: Artwork (AI for drafts, me for production-quality deliverables)</h3>
        <p>
          Once the wireframe was stable and tested, I moved to visuals.
        </p>
        <p>
          Normally I'd hand-draw everything. But since this is literally a guide about AI, I let AI generate initial concept art—then I did the part AI still struggles with: <strong>making it production-ready.</strong>
        </p>
        <p>
          It took a few tries but I found a prompt that got the results I was looking for and I used that as a template for each sprite (illustration).
        </p>
        <p>
          My eventual illustration prompt:
        </p>
        <div class="code" style="white-space:normal;" data-label="Illustration prompt">
          Full-body pixel-art 2D fighting game sprite of ChatGPT as a grandmaster chef.<br>
          Stands tall and centered in a confident neutral stance, one arm extended in invitation,
          the other resting calmly at the waist.<br><br>

          Wears a pristine white long-coat chef uniform with teal inner lining and trim.<br>
          Clothing is immaculate and minimal. Glowing kitchen utensils floats horizontally
          at chest height.<br><br>

          Calm expression, perfectly balanced posture.<br>
          Color palette strictly white, teal, and soft gray.<br>
          Strong, clean silhouette.
        </div>
        <p>
          Refining prompt (first pass is almost always a draft even in an established workflow):
        </p>
        <div class="code" style="white-space:normal;" data-label="Refining prompt">
          subtle forward lean, knees slightly bent, grounded stance<br>
          visible tension in shoulders and forearms<br>
          calm but alert posture, like a martial arts ready stance instead of statue-like
        </div>
        <p>
          Then I cleaned everything up in Photoshop and converted it into actual pixel-art sprites. Here's the progression for the ChatGPT chef—from the initial AI generation, through the refined prompt, to the final pixel-art sprite:
        </p>

        <div id="spriteProgression">
          <div class="sprite-carousel">
            <button class="carousel-arrow" id="spritePrev" type="button" aria-label="Previous slide">&#8249;</button>
            <div class="roster-carousel">
              <figure class="diagram" style="margin:0;">
                <img src="../images/character_select/ChatGPT_v1.png" alt="ChatGPT chef v1: initial AI-generated illustration—stiff pose, static, clean but lifeless" loading="lazy">
                <figcaption><strong>v1—Initial prompt.</strong> A nice start but too stiff, no energy, and also <em>not</em> pixel-art. There were a few more attempts to achieve the pixel-art style before I gave up.</figcaption>
              </figure>
              <figure class="diagram" style="margin:0;">
                <img src="../images/character_select/ChatGPT_v2.png" alt="ChatGPT chef v2: refined prompt—more dynamic stance, dramatic lighting, closer to fighting-game energy" loading="lazy">
                <figcaption><strong>v2—Refined prompt.</strong> More dynamic and closer to the fighting-game energy I was looking for. It still wasn't pixel-art but I already knew this process would need lots of manual cleanup.</figcaption>
              </figure>
              <figure class="diagram" style="margin:0;">
                <img src="../images/character_select/ChatGPT.png" alt="ChatGPT chef final: hand-cleaned pixel art sprite with floating utensils, clean silhouette, and intentional highlights" loading="lazy">
                <figcaption><strong>Final—Pixel-art sprite.</strong> Downscaled and hand-cleaned in Photoshop. Stray pixels removed, silhouette tightened, highlights and floating utensils added manually.</figcaption>
              </figure>
            </div>
            <button class="carousel-arrow" id="spriteNext" type="button" aria-label="Next slide">&#8250;</button>
          </div>
          <nav class="carousel-dots" aria-label="Sprite progression slides">
            <button class="carousel-dot active" type="button" aria-label="Slide 1: initial prompt"></button>
            <button class="carousel-dot" type="button" aria-label="Slide 2: refined prompt"></button>
            <button class="carousel-dot" type="button" aria-label="Slide 3: final sprite"></button>
          </nav>
        </div>
        <div class="grid callout">
          <div>
            <h4>Sprite cleanup process</h4>
            <ol>
              <li>
                <strong>Pick a target sprite size</strong>
                <ul><li>128×128</li></ul>
              </li>
              <li>
                <strong>Downscale intentionally</strong>
                <ul>
                  <li>Nearest Neighbor interpolation (no smoothing)</li>
                </ul>
              </li>
              <li>
                <strong>Manual cleanup</strong>
                <ul>
                  <li>Clean silhouettes</li>
                  <li>Fix eyes/hands/tools</li>
                  <li>Remove stray pixels and accidental anti-aliasing</li>
                  <li>Add intentional highlights + shadows</li>
                </ul>
              </li>
              <li>
                <strong>Resize to target resolution</strong>
              </li>
            </ol>
          </div>
          <div>
            <!--Image Comparison Slider-->
            <div>
              <figure dir="rtl">
                <p class="image-compare" role="img" tabindex="0" aria-label="Interactive comparison of Gemini sprite before and after pixel-art cleanup." aria-description="Two overlaid images are revealed by a slider. One shows the final hand-cleaned pixel-art Gemini chef sprite. The other shows the initial AI-generated illustration before cleanup.">
                  <span>
                    <img src="../images/character_select/Gemini.png" alt="" aria-hidden="true" loading="lazy" class="image-compare" style="background-color: var(--bg2); border-radius: 8px;">
                    </span>
                    <img src="../images/character_select/Gemini_v1.png" alt="" aria-hidden="true" loading="lazy">
                </p>
              </figure>
            </div>

          </div>
        </div>
        <p>
          For headshots, I took the pragmatic route: <strong>cropped close-ups of the full sprite</strong> instead of drawing separate portraits.
        </p>
        <p>
          In the end, each sprite took <strong>2-3 hours</strong> from prompt to cleanup, which actually increased the time compared to drawing from scratch (for this level of quality). That said, each sprite went a little quicker as the prompts and workflows were developed—so it <em>might</em> be around 20% faster than manually drawing if I needed to do <strong>this exact same process</strong> again. Due to the time constraint, I decided to scrap my plans to animate each sprite and accepted that the quality was slightly lower than my usual standard.
        </p>
        <div class="grid">
          <div>
            <h3>Phase 4: Bios (AI drafts, human rewrite)</h3>
            <p>
              For the character bios, I prompted AI to generate funny, exaggerated writeups for each model, including:
            </p>
            <ul>
              <li><strong>Best at:</strong> what this model tends to excel at</li>
              <li><strong>Watch for:</strong> the model's common failures</li>
            </ul>
            <p>
              The first drafts gave me baselines to work with and I checked each against different models and a quick internet search to see if anything was wildly inaccurate, but nothing went in unedited. Some needed a light polish while others needed to be completely rewritten. In the end, around 70% of the written content was revised from the final AI output. 
            </p>    
          </div>
          <div>
            <h3>Phase 5: Integration + polish</h3>
            <p>
              Once assets and copy existed, I dropped them into folders, hooked them up to the data model, and used AI to help map content into the wireframe (and made a few manual adjustments).
            </p>
            <p>
              After that it was pure polish:
            </p>
            <ul>
              <li>Spacing and alignment tweaks</li>
              <li>Animations</li>
              <li>Interaction timing</li>
              <li>Making the select screen feel responsive</li>
              <li>Endlessly modifying the stats (which are just for fun—don't take them too seriously)</li>
            </ul>
          </div>
        </div>
        <div class="truth">
          <strong>And now the moment you've been waiting for:</strong> the character select feature is finally finished and ready to ship...
        </div>
      </section>

      <section>

        <h2>Choose Your Chef</h2>
        <p>
          Now that you know how the kitchen works, it's time to meet the roster. Each model has different strengths, weaknesses, and wildly overdramatic cooking styles—and the right pick depends on what you're making (and what kitchen you're in).
        </p>

        <div class="sandbox-card" id="chefSelect">
          <label class="sandbox-label"><strong>Select your chef</strong></label>
          <div class="select-screen">
            <div class="portrait-row" id="csRoster"></div>
            <div class="chef-panel" id="csPanel">
              <div class="chef-art" id="csArt"></div>
              <div class="chef-info" id="csInfo"></div>
            </div>
          </div>
        </div>
      </section>

      <section>
        <h2>Final_Final_v6: What AI actually did for me</h2>
        <p>
          AI didn’t build the feature for me. It accelerated the parts that would usually slow me down:
        </p>
        <ul>
          <li>Surfacing missing requirements early</li>
          <li>Generating first drafts (art and copy)</li>
          <li>Acting as a review pass when I wanted an extra set of eyes</li>
        </ul>
        <p>
          The key is that the workflow still has a human checkpoint at every stage: I decide what moves to the next stage, and I verify <em>everything</em> before it’s published.
        </p>
        <p>
          AI was most effective in the planning and drafting stages. It cut huge amounts of time by helping me organize my thoughts, build checklists, and generate placeholder text and images. I say “placeholder” intentionally—very little of what it produced went straight into the final version without heavy editing, and around 70% of it was rewritten or replaced entirely. Overall, AI nearly doubled my speed during early planning but I didn't see any measurable increase during production. Some AI-induced detours actually slowed me down, but with a more dialed-in process, the “production” gain could reach close to 20%.
        </p>
        <p>
          This workflow worked well for me because I’m experienced with each part of this type of project (planning, coding, writing, and design) and already comfortable with AI prompting. Even so, a significant portion of the work still required a human touch.
        </p>
        <div class="truth">
          <strong>Final takeaways from this project:</strong> AI can accelerate parts of a project, but it doesn’t shorten every phase equally. It’s most effective when it’s part of an established process—paired with prompts that are pressure-tested, refined, and can be reused reliably.
        </div>
      </section>

    </main>
    <footer class="sub glass" style="grid-column: 1 / -1">
      <span>© 2026 – Demystifying AI: Stepping into the LLM Kitchen by Chris Yasuda Drysdale</span>
    </footer>
  </div>

  <button id="tocFab" class="toc-fab" aria-controls="tocList" aria-expanded="false"
    aria-label="Open table of contents">☰ Inventory</button>
  <a id="toTop" href="#top" title="Back to top">&#8593;</a>

  <script>
    // Sprite progression carousel
    (() => {
      const track = document.querySelector('#spriteProgression .roster-carousel');
      if (!track) return;
      const dots = [...document.querySelectorAll('#spriteProgression .carousel-dot')];
      const slides = [...track.querySelectorAll('.diagram')];
      const prev = document.getElementById('spritePrev');
      const next = document.getElementById('spriteNext');
      if (!dots.length || !slides.length) return;

      let ticking = false;

      function getIndex() {
        // Each slide is centered; measure from the track's visual center
        const center = track.scrollLeft + track.offsetWidth / 2;
        let closest = 0, minDist = Infinity;
        slides.forEach((s, i) => {
          const mid = s.offsetLeft + s.offsetWidth / 2;
          const d = Math.abs(center - mid);
          if (d < minDist) { minDist = d; closest = i; }
        });
        return closest;
      }

      function update() {
        const idx = getIndex();
        dots.forEach((d, i) => d.classList.toggle('active', i === idx));
        if (prev) prev.disabled = idx === 0;
        if (next) next.disabled = idx === slides.length - 1;
        ticking = false;
      }

      function scrollTo(i) {
        const slide = slides[i];
        if (!slide) return;
        const target = slide.offsetLeft - (track.offsetWidth - slide.offsetWidth) / 2;
        track.scrollTo({ left: target, behavior: 'smooth' });
      }

      track.addEventListener('scroll', () => {
        if (!ticking) { ticking = true; requestAnimationFrame(update); }
      }, { passive: true });

      dots.forEach((dot, i) => dot.addEventListener('click', () => scrollTo(i)));
      if (prev) prev.addEventListener('click', () => { const i = getIndex(); if (i > 0) scrollTo(i - 1); });
      if (next) next.addEventListener('click', () => { const i = getIndex(); if (i < slides.length - 1) scrollTo(i + 1); });

      update();
    })();

    // Predict-like-a-phone game
    (() => {
      const game = document.getElementById('predictGame');
      if (!game) return;

      const rounds = [
        {
          s: 'I need to go to the ___',
          c: ['store', 'quantum', 'briefly'],
          p: 0, b: 0,
          e: 'For common phrases, simple prediction nails it. "To the" \u2192 "store" is one of the most frequent word pairings in English.'
        },
        {
          s: 'The surgeon carefully picked up the ___',
          c: ['phone', 'scalpel', 'kids'],
          p: 0, b: 1,
          e: 'Your phone sees "up the" and guesses "phone"\u2014a common pairing. An LLM sees "surgeon" earlier on the counter and picks "scalpel."'
        },
        {
          s: 'After years of studying marine biology, she finally got to swim with the ___',
          c: ['flow', 'kids', 'dolphins'],
          p: 1, b: 2,
          e: '"Marine biology" is 9 words back\u2014far beyond the phone\u2019s window, but easily within an LLM\u2019s counter space.'
        },
        {
          s: 'Of all the planets in our solar system, Jupiter is the ___',
          c: ['planet', 'same', 'largest'],
          p: 1, b: 2,
          e: 'Your phone sees "is the" and suggests "same"\u2014one of the most common bigrams in English. An LLM connects "planets," "solar system," and "Jupiter" to pick "largest."'
        }
      ];

      let cur = 0;
      let done = false;
      const history = [];

      const sentenceEl = game.querySelector('#pgSentence');
      const choicesEl = game.querySelector('#pgChoices');
      const revealEl = game.querySelector('#pgReveal');
      const nextBtn = game.querySelector('#pgNext');
      const backBtn = game.querySelector('#pgBack');
      const resetBtn = game.querySelector('#pgReset');
      const progressEl = game.querySelector('#pgProgress');

      function buildRevealHTML(r) {
        let html = '';
        if (r.p === r.b) {
          html += '<p style="margin:0"><strong>Phone suggestion: ' + r.c[r.p] + '</strong>\u2014matches the context-aware pick this time.</p>';
        } else {
          html += '<p style="margin:0"><strong>Phone suggestion:</strong> ' + r.c[r.p] + ' &nbsp;|&nbsp; <strong>Context-aware pick:</strong> ' + r.c[r.b] + '</p>';
        }
        html += '<p class="note" style="margin:.5rem 0 0">' + r.e + '</p>';
        if (cur >= rounds.length - 1) {
          html += '<p style="margin:.8rem 0 0"><strong>Takeaway:</strong> Simple prediction matches common patterns. LLMs use attention to match meaning\u2014which is why they\u2019re dramatically better, but still one token at a time.</p>';
        }
        return html;
      }

      function showAnswered(r) {
        done = true;
        const btns = choicesEl.querySelectorAll('button');
        btns.forEach((btn, i) => {
          btn.disabled = true;
          if (i === r.b) {
            btn.style.outline = '2px solid var(--accent)';
            btn.style.outlineOffset = '2px';
          }
        });
        revealEl.innerHTML = buildRevealHTML(r);
        revealEl.style.display = 'block';
      }

      function render() {
        const r = rounds[cur];
        sentenceEl.textContent = r.s;
        choicesEl.innerHTML = '';
        progressEl.textContent = 'Round ' + (cur + 1) + ' of ' + rounds.length;

        r.c.forEach((word, i) => {
          const btn = document.createElement('button');
          btn.className = 'sandbox-btn';
          btn.textContent = word;
          btn.type = 'button';
          btn.addEventListener('click', () => pick(i));
          choicesEl.appendChild(btn);
        });

        if (history[cur] !== undefined) {
          showAnswered(r);
          nextBtn.style.display = cur < rounds.length - 1 ? 'inline-block' : 'none';
        } else {
          done = false;
          revealEl.style.display = 'none';
          nextBtn.style.display = 'none';
        }

        backBtn.style.display = cur > 0 ? 'inline-block' : 'none';
        resetBtn.style.display = history.length > 0 ? 'inline-block' : 'none';
      }

      function pick(idx) {
        if (done) return;
        history[cur] = idx;
        showAnswered(rounds[cur]);
        if (cur < rounds.length - 1) {
          nextBtn.style.display = 'inline-block';
        }
        resetBtn.style.display = 'inline-block';
      }

      nextBtn.addEventListener('click', () => { cur++; render(); });
      backBtn.addEventListener('click', () => { cur--; render(); });
      resetBtn.addEventListener('click', () => { cur = 0; history.length = 0; render(); });

      render();
    })();

    // Temperature demo slider
    (() => {
      const slider = document.getElementById('tempSlider');
      const output = document.getElementById('tempOutput');
      const label  = document.getElementById('tempValue');
      if (!slider || !output || !label) return;

      const tiers = [
        [12,  ' blue'],
        [25,  ' clear and cloudless'],
        [37,  ' a deep, brilliant blue that makes the whole city look painted'],
        [50,  ' an ocean flipped upside down, restless and wide'],
        [62,  ' humming with colors that haven\'t been named yet'],
        [75,  ' bargaining with the sea over who gets to keep the color indigo'],
        [87,  ' a cathedral ceiling, echoing with wind'],
        [99,  ' a slow-breathing gradient, deepening toward dusk'],
        [100, ' melting into purple dreams, gathering passing secrets in invisible hands']
      ];

      slider.addEventListener('input', () => {
        const v = +slider.value;
        label.textContent = 'Temperature: ' + (v / 50).toFixed(1);
        const tier = tiers.find(t => v <= t[0]) || tiers[tiers.length - 1];
        output.textContent = tier[1];
      });
    })();

    // Context overflow demo (kitchen-themed)
    (() => {
      const demo = document.getElementById('overflowDemo');
      if (!demo) return;

      const scene    = document.getElementById('ofScene');
      const addBtn   = document.getElementById('ofAdd');
      const resendBtn = document.getElementById('ofResend');
      const resetBtn = document.getElementById('ofReset');
      const status   = document.getElementById('ofStatus');
      const reveal   = document.getElementById('ofReveal');

      const MAX = 8;
      const SLOT_WIDTH = 11.5; // percent per slot (ticket + gap)
      const START_OFFSET = 1.5; // percent from left edge
      let items = [];
      let msgCount = 0;
      let rulesDropped = false;
      let falling = false;

      function ticketLeft(i) {
        return START_OFFSET + i * SLOT_WIDTH;
      }

      function twoLine(text) {
        // Split label into two lines at the space: "Your prompt" → "Your<br>prompt"
        var idx = text.indexOf(' ');
        return idx > 0 ? text.slice(0, idx) + '<br>' + text.slice(idx + 1) : text;
      }

      function render(fallingIndex, newIndex) {
        scene.innerHTML = '';
        items.forEach((item, i) => {
          const el = document.createElement('div');
          const isPrompt = item.type === 'rules';
          const isFalling = i === fallingIndex;
          const isNew = i === newIndex;
          el.className = 'kitchen-ticket'
            + (isPrompt ? ' kitchen-ticket--prompt' : ' kitchen-ticket--msg')
            + (isFalling ? ' kitchen-ticket--falling' : '');
          el.style.left = ticketLeft(i) + '%';

          // Only the new ticket gets the drop-in animation; others skip it
          if (!isNew && !isFalling) {
            el.style.animation = 'none';
            el.setAttribute('data-settled', '');
          }

          const label = document.createElement('span');
          label.className = 'kitchen-ticket__label';
          label.innerHTML = twoLine(item.label);
          el.appendChild(label);

          // After drop-in animation, mark as settled for smooth left transitions
          if (isNew) {
            el.addEventListener('animationend', function() {
              el.setAttribute('data-settled', '');
            }, { once: true });
          }

          scene.appendChild(el);
        });
        status.textContent = items.length + ' / ' + MAX + ' slots used';
      }

      function init() {
        items = [{ label: 'Your prompt', type: 'rules' }];
        msgCount = 0;
        rulesDropped = false;
        falling = false;
        reveal.style.display = 'none';
        resendBtn.style.display = 'none';
        addBtn.disabled = false;
        render(-1, 0); // animate the initial prompt ticket dropping in
      }

      addBtn.addEventListener('click', () => {
        if (falling) return; // guard against rapid clicks during fall
        msgCount++;

        if (items.length >= MAX) {
          const removed = items.shift();

          if (removed.type === 'rules' && !rulesDropped) {
            rulesDropped = true;
            falling = true;

            // Re-insert prompt at index 0 so we can animate it falling
            items.unshift(removed);
            items.push({ label: 'Msg ' + msgCount, type: 'msg' });
            render(0, items.length - 1); // index 0 falls, last index drops in

            // After fall animation (ticketFallOff = 1.2s), clean up and show reveal
            setTimeout(() => {
              items.shift(); // remove prompt for real
              falling = false;
              render(-1, -1); // no animations on cleanup re-render
              reveal.innerHTML = '<p style="margin:0"><strong>Your original prompt just fell off the counter.</strong></p><p class="note" style="margin:.5rem 0 0">The model has no idea it ever existed. This is one of the ways context overflow can happen. The original instructions are physically gone.</p>';
              reveal.style.display = 'block';
              resendBtn.style.display = 'inline-block';
              addBtn.disabled = (msgCount >= MAX + 5);
            }, 1250);
            return;
          }
        }

        items.push({ label: 'Msg ' + msgCount, type: 'msg' });
        render(-1, items.length - 1); // animate only the new ticket
        if (msgCount >= MAX + 5) addBtn.disabled = true;
      });

      resendBtn.addEventListener('click', () => {
        if (falling) return;
        // Push the prompt back onto the line as a new ticket (bumps oldest msg off)
        if (items.length >= MAX) items.shift();
        items.push({ label: 'Your prompt', type: 'rules' });
        render(-1, items.length - 1);
        resendBtn.style.display = 'none';
        reveal.innerHTML = '<p style="margin:0"><strong>Your prompt is back on the counter.</strong></p><p class="note" style="margin:.5rem 0 0">In a long conversation, restating your key instructions reminds the model what matters. Think of it as handing the chef a fresh copy of the original order. It costs you context space, but it keeps the output aligned with your intent\u2014especially when earlier messages have already been lost to overflow.</p>';
      });

      resetBtn.addEventListener('click', init);
      init();
    })();

    // Hallucination quiz
    (() => {
      const quiz = document.getElementById('hallucinationQuiz');
      if (!quiz) return;

      const rounds = [
        {
          claim: 'The average person swallows approximately eight spiders per year while sleeping.',
          fact: false,
          e: 'The origin of this "fact" is a mystery. It was once presented as “Real Fact” #31 under Snapple caps, but isn\u2019t listed among the "Real Facts" on their website. It\u2019s also widely attributed to a 1993 magazine column to demonstrate how easily misinformation spreads, however the article itself cannot be found anywhere online.'
        },
        {
          claim: 'A group of flamingos is called a "flamboyance."',
          fact: true,
          e: 'Correct\u2014like "a murder of crows" or "a parliament of owls." It sounds made up, which is the point: how plausible something sounds tells you nothing about whether it\u2019s true.'
        },
        {
          claim: 'Goldfish have a memory span of approximately three seconds.',
          fact: false,
          e: 'Goldfish can retain learned behaviors for months and navigate mazes. This myth is so widespread and goes back so far that there\u2019s no known origin to this myth and many models often state it as fact without any hedging.'
        },
        {
          claim: 'Bananas are technically classified as berries, but strawberries are not.',
          fact: true,
          e: 'Botanically correct. Berries develop from a single ovary\u2014bananas qualify, strawberries don\u2019t. While LLMs might not know how many R\u2019s or N\u2019s are in those fruits (due to how tokenization works) they get this right because it\u2019s well-represented in training data.'
        }
      ];

      let cur = 0;
      let done = false;
      const history = [];

      const claimEl = document.getElementById('hqClaim');
      const factBtn = document.getElementById('hqFact');
      const mythBtn = document.getElementById('hqMyth');
      const revealEl = document.getElementById('hqReveal');
      const nextBtn = document.getElementById('hqNext');
      const backBtn = document.getElementById('hqBack');
      const resetBtn = document.getElementById('hqReset');
      const progressEl = document.getElementById('hqProgress');

      function computeScore() {
        let s = 0;
        for (let i = 0; i < history.length; i++) {
          if (history[i] === rounds[i].fact) s++;
        }
        return s;
      }

      function buildRevealHTML(r, answeredFact) {
        const correct = answeredFact === r.fact;
        let html = '<p style="margin:0">';
        html += correct ? '<strong>Correct!</strong> ' : '<strong>Not quite.</strong> ';
        html += 'This is <strong>' + (r.fact ? 'a real fact' : 'a common myth') + '</strong>.</p>';
        html += '<p class="note" style="margin:.5rem 0 0">' + r.e + '</p>';
        if (cur >= rounds.length - 1) {
          const score = computeScore();
          html += '<p style="margin:.8rem 0 0"><strong>Result: ' + score + ' / ' + rounds.length + '.</strong> ';
          if (score === rounds.length) {
            html += 'Nice work\u2014but in practice there\u2019s no "Fact or Myth" button. Every AI claim arrives in the same confident voice.';
          } else {
            html += 'That\u2019s the point. Every claim used the same confident tone. The myths were structurally identical to the real facts.';
          }
          html += '</p>';
        }
        return html;
      }

      function showAnswered(r, answeredFact) {
        done = true;
        factBtn.disabled = true;
        mythBtn.disabled = true;
        factBtn.style.outline = '';
        mythBtn.style.outline = '';
        const rightBtn = r.fact ? factBtn : mythBtn;
        rightBtn.style.outline = '2px solid var(--accent)';
        rightBtn.style.outlineOffset = '2px';
        revealEl.innerHTML = buildRevealHTML(r, answeredFact);
        revealEl.style.display = 'block';
      }

      function render() {
        const r = rounds[cur];
        claimEl.textContent = '\u201c' + r.claim + '\u201d';
        progressEl.textContent = 'Claim ' + (cur + 1) + ' of ' + rounds.length;

        if (history[cur] !== undefined) {
          showAnswered(r, history[cur]);
          nextBtn.style.display = cur < rounds.length - 1 ? 'inline-block' : 'none';
        } else {
          done = false;
          factBtn.disabled = false;
          mythBtn.disabled = false;
          factBtn.style.outline = '';
          mythBtn.style.outline = '';
          revealEl.style.display = 'none';
          nextBtn.style.display = 'none';
        }

        backBtn.style.display = cur > 0 ? 'inline-block' : 'none';
        resetBtn.style.display = history.length > 0 ? 'inline-block' : 'none';
      }

      function pick(answeredFact) {
        if (done) return;
        history[cur] = answeredFact;
        showAnswered(rounds[cur], answeredFact);
        if (cur < rounds.length - 1) {
          nextBtn.style.display = 'inline-block';
        }
        resetBtn.style.display = 'inline-block';
      }

      factBtn.addEventListener('click', () => pick(true));
      mythBtn.addEventListener('click', () => pick(false));
      nextBtn.addEventListener('click', () => { cur++; render(); });
      backBtn.addEventListener('click', () => { cur--; render(); });
      resetBtn.addEventListener('click', () => { cur = 0; history.length = 0; render(); });

      render();
    })();

    // Prompt iteration demo
    (() => {
      const demo = document.getElementById('iterationDemo');
      if (!demo) return;

      const steps = [
      {
        prompt: 'Write me an email about the product update.',
        output:
          'Subject: Product Update\n\nHi there,\n\nI wanted to share a quick update about our product. We\u2019ve been working hard and have made some exciting improvements. More details coming soon... hopefully.\n\nLet me know if you have any questions.\n\nBest,\n[Company name]',
        notice:
          '<strong>The problem:</strong> The LLM had almost nothing to work with, so the output is generic filler. It contains no actual product details, no hook, and no reason to buy. But notice\u2014the email <em>looks</em> professional and well-structured. If you weren\u2019t paying attention, you might actually send this and wonder why nobody clicked.'
      },
      {
        prompt:
          'Write a marketing email announcing the iRock Pro MAX+ pet rock. It now has integrated AI features and voice commands. Mention rock-solid answers and that pre-orders are open.',
        output:
          'Subject: Introducing iRock Pro MAX+ \u2014 Your Pet Rock, Now with AI\n\nHi there,\n\nWe\u2019re excited to introduce the iRock Pro MAX+, a pet rock with integrated AI features. It offers rock-solid answers and voice command support.\n\nPre-orders are now open. If you\u2019d like to be among the first to experience a smarter pet rock, reserve yours today.\n\nBest,\n[Company name]',
        notice:
          '<strong>Better:</strong> Now the email has <em>some</em> real content because the prompt provided actual features and a basic CTA. But the tone is still generic and there\u2019s no offer, urgency, or specific feature framing (why should anyone care about a rock with AI?).'
      },
      {
        prompt:
          'Write a marketing email announcing our new product release.\n\nProduct name: iRock Pro MAX+\nTagline: \u201cAI-POWERED SMART ROCK!\u201d\nTone: sophisticated tech-hype + conversational \nFormat: 3 short paragraphs + a feature bullet list + a clear CTA section.\nInclude features in the attached iRock_Pro_Max_Plus.doc (use the exact phrases at least once).\n\nOffer: 20% off pre-orders with code STONESMART\nDeadline: 11:59PM Friday\nCTA button text (as plain text): [Preorder iRock Pro MAX+]\nAdd the iRock standard disclaimer\nEnd with a \u201cWhat to do next\u201d bullet list (3 bullets).\nInclude the attached hero image at the top.',
        output:
          'Subject: Meet iRock Pro MAX+ \u2014 the AI-POWERED SMART ROCK!'+
          '<img src="../images/iRock-Pro-Max-hero.png" alt="iRock Pro MAX+ email hero image" style="width:100%;border-radius:6px;margin-bottom:8px">'+
          'Hi [first_name],\n\nThe future just landed on your desk. Introducing iRock Pro MAX+, the world\u2019s first pet rock upgraded with cutting edge artificial intelligence. It\u2019s cute, it\u2019s quiet, and it seamlessly connects to your smart home.\n\nPowered by Stone-Cold Intelligence\u2122 and an impressive 3000 petaflops processing, iRock Pro MAX+ delivers rock-solid answers. It also includes Ultra 5G-lithic connectivity plus our proprietary R.O.C.K. (Robotic Optimized Cognitive Kernel) to keep your conversations grounded.\n\nFeatures you never asked for:\n\u2022 Stone-Cold Intelligence\u2122\n\u2022 3000 petaflops processing\n\u2022 Rock-solid answers\n\u2022 Ultra 5G-lithic connectivity\n\u2022 R.O.C.K. (Robotic Optimized Cognitive Kernel)\n\u2022 Voice command enabled\n\u2022 Virtual pet rock mode\n\u2022 Zen mode setting\n\nPre-orders are open now\u2014and for a limited time you can get 20% off with code STONESMART (expires 11:59PM Friday).\n\n[Preorder iRock Pro MAX+]\n\nFine print: AI features vary by subscription tier. Plans start at $29.99/mo for Basic (includes Zen mode)\n\nWhat to do next:\n\u2022 Use code STONESMART to pre-order by 11:59PM Friday\n\u2022 Log in to our app to name your iRock Pro MAX+\n\u2022 Enable Zen mode and experience pure, uninterrupted nothingness\n',
        notice:
          '<strong>Same framework, different story.</strong> The only difference was the ticket: specific product facts, a structured format, and explicit tone produced an email that\u2019s usable. Three prompts, three drafts, one extremely useful rock.'
      }
      ];

      let cur = 0;

      const promptEl = document.getElementById('itPrompt');
      const outputEl = document.getElementById('itOutput');
      const noticeEl = document.getElementById('itNotice');
      const nextBtn = document.getElementById('itNext');
      const backBtn = document.getElementById('itBack');
      const resetBtn = document.getElementById('itReset');
      const progressEl = document.getElementById('itProgress');
      const promptBlock = document.getElementById('itPromptBlock');

      function render() {
        const s = steps[cur];
        progressEl.textContent = 'Draft ' + (cur + 1) + ' of ' + steps.length;
        promptBlock.setAttribute('data-label', 'Order ticket (draft ' + (cur + 1) + '):');
        promptEl.textContent = s.prompt;
        outputEl.innerHTML = s.output.replace(/\n/g, '<br>');
        noticeEl.innerHTML = '<p style="margin:0">' + s.notice + '</p>';
        nextBtn.style.display = cur < steps.length - 1 ? 'inline-block' : 'none';
        backBtn.style.display = cur > 0 ? 'inline-block' : 'none';
        resetBtn.style.display = cur > 0 ? 'inline-block' : 'none';
      }

      nextBtn.addEventListener('click', () => { cur++; render(); });
      backBtn.addEventListener('click', () => { cur--; render(); });
      resetBtn.addEventListener('click', () => { cur = 0; render(); });

      render();
    })();

    // Workflow station demo
    (() => {
      const demo = document.getElementById('workflowDemo');
      if (!demo) return;

      const steps = [
        {
          label: 'Station 1: Organize',
          prompt: 'Here are rough notes from today\u2019s product sync. Clean them up: fix shorthand, group related items by topic, and mark action items with \u2192.',
          output: 'DESIGN\n\u2022 Homepage redesign approved \u2014 using Option B (card layout)\n\u2022 Mobile nav needs another round \u2014 Janet reviewing by Thursday\n\nENGINEERING\n\u2022 API migration 80% complete, on track for March 7\n\u2022 Auth module blocked on third-party SDK update\n\u2192 Sam to contact vendor re: SDK timeline (by Wed)\n\nLAUNCH\n\u2022 QA starts Monday \u2014 test plan in Confluence\n\u2192 All leads review QA sections (before Monday standup)\n\u2192 3 volunteers needed for user testing (sign up by EOD Fri)',
          notice: 'This station has one job: organizing raw input. The chef isn\u2019t writing the summary yet\u2014just sorting ingredients so the next station starts clean.'
        },
        {
          label: 'Station 2: Extract',
          prompt: 'From the organized notes above, create two lists: (1) Key decisions, one line each. (2) Action items with owner and deadline.',
          output: 'DECISIONS\n1. Homepage redesign \u2192 Option B (card layout)\n2. Mobile nav deferred for another review round\n3. API migration on track for March 7 deadline\n4. QA testing launches Monday\n\nACTION ITEMS\n\u2022 Sam \u2192 Contact SDK vendor re: timeline (by Wed)\n\u2022 All leads \u2192 Review QA test plan sections (before Mon standup)\n\u2022 TBD \u2192 3 volunteers for user testing (by EOD Fri)',
          notice: 'The chef is working with clean, structured input from Station 1\u2014not the original messy shorthand. Better input at each station means better output at every station.'
        },
        {
          label: 'Station 3: Draft + Expo',
          prompt: 'Write a 3-paragraph stakeholder summary from these decisions and action items. Tone: concise, professional. After the summary, add a "Verify before sending" section listing anything implied but not explicitly confirmed in the original notes.',
          output: 'PRODUCT SYNC \u2014 STAKEHOLDER SUMMARY\n\nThe homepage redesign is moving forward with Option B (card layout). Mobile navigation needs an additional review round; Janet will have feedback by Thursday.\n\nThe API migration is 80% complete, tracking toward March 7. The auth module is blocked by a third-party SDK update\u2014Sam is following up with the vendor by Wednesday.\n\nQA begins Monday. Leads should review their test plan sections before standup. We need three volunteers for user testing by end of day Friday.\n\nVERIFY BEFORE SENDING\n\u2022 Was Option B formally approved or just preferred?\n\u2022 Is March 7 a hard deadline or a target?\n\u2022 Is there a sign-up channel for user testing volunteers?',
          notice: '<strong>Three stations, three focused prompts.</strong> Each received clean input from the previous step, had a single clear job, and produced structured output. The "Verify" section is the expo pass\u2014the chef flagging items for you to check before serving.'
        }
      ];

      let cur = 0;

      const promptEl = document.getElementById('wfPrompt');
      const outputEl = document.getElementById('wfOutput');
      const noticeEl = document.getElementById('wfNotice');
      const nextBtn = document.getElementById('wfNext');
      const backBtn = document.getElementById('wfBack');
      const resetBtn = document.getElementById('wfReset');
      const progressEl = document.getElementById('wfProgress');
      const promptBlock = document.getElementById('wfPromptBlock');

      function render() {
        const s = steps[cur];
        progressEl.textContent = s.label;
        promptBlock.setAttribute('data-label', s.label + ' \u2014 prompt:');
        promptEl.textContent = s.prompt;
        outputEl.innerHTML = s.output.replace(/\n/g, '<br>');
        noticeEl.innerHTML = '<p style="margin:0">' + s.notice + '</p>';
        nextBtn.style.display = cur < steps.length - 1 ? 'inline-block' : 'none';
        backBtn.style.display = cur > 0 ? 'inline-block' : 'none';
        resetBtn.style.display = cur > 0 ? 'inline-block' : 'none';
      }

      nextBtn.addEventListener('click', () => { cur++; render(); });
      backBtn.addEventListener('click', () => { cur--; render(); });
      resetBtn.addEventListener('click', () => { cur = 0; render(); });

      render();
    })();
    // Wireframe demo (appendix)
    (() => {
      const root = document.getElementById('tplChefSelect');
      if (!root) return;

      const rosterEl = document.getElementById('tplRoster');
      const artEl    = document.getElementById('tplArt');
      const infoEl   = document.getElementById('tplInfo');

      const DATA = [
        { id: 'item-1', name: 'Lorem Ipsum',        title: 'Lorem ipsum dolor',    bio: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.', accent: '#629c67', stats: { PREP: 1, FLAV: 1, HEAT: 1, GEAR: 1, PLAT: 1 } },
        { id: 'item-2', name: 'Dolor Sit',          title: 'Consectetur adipiscing', bio: 'Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.',                  accent: '#5f8fb0', stats: { PREP: 1, FLAV: 1, HEAT: 1, GEAR: 1, PLAT: 1 } },
        { id: 'item-3', name: 'Amet Elit',          title: 'Sed do eiusmod',        bio: 'Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.',                        accent: '#9f79d4', stats: { PREP: 1, FLAV: 1, HEAT: 1, GEAR: 1, PLAT: 1 } },
        { id: 'item-4', name: 'Tempor Incididunt',  title: 'Ut labore et',          bio: 'Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.',               accent: '#4d7cc2', stats: { PREP: 1, FLAV: 1, HEAT: 1, GEAR: 1, PLAT: 1 } },
        { id: 'item-5', name: 'Magna Aliqua',       title: 'Ut enim ad',            bio: 'Curabitur pretium tincidunt lacus. Nulla gravida orci a odio. Nullam varius, turpis et commodo pharetra.',                      accent: '#cf6448', stats: { PREP: 1, FLAV: 1, HEAT: 1, GEAR: 1, PLAT: 1 } },
        { id: 'item-6', name: 'Commodo Consequat',  title: 'Duis aute irure',       bio: 'Integer in mauris eu nibh euismod gravida. Duis ac tellus et risus vulputate vehicula. Donec lobortis risus a elit.',            accent: '#7b8f4f', stats: { PREP: 1, FLAV: 1, HEAT: 1, GEAR: 1, PLAT: 1 } }
      ];

      let active = 0;

      function stars(n) {
        return '<span class="star-on">\u2605</span>'.repeat(n) + '<span class="star-off">\u2606</span>'.repeat(5 - n);
      }

      function render() {
        var item = DATA[active];
        root.style.setProperty('--chef-accent', item.accent);

        // Roster
        rosterEl.innerHTML = '';
        DATA.forEach(function (d, i) {
          var btn = document.createElement('button');
          btn.className = 'portrait' + (i === active ? ' active' : '');
          btn.type = 'button';
          btn.setAttribute('aria-label', 'Select ' + d.name);
          btn.textContent = 'Lorem ' + (i + 1);
          btn.addEventListener('click', function () { active = i; render(); });
          rosterEl.appendChild(btn);
        });

        // Art placeholder
        artEl.innerHTML = '';
        var ph = document.createElement('div');
        ph.className = 'chef-art-placeholder';
        ph.textContent = 'Lorem ipsum';
        artEl.appendChild(ph);

        // Info
        var html = '<div class="chef-name">' + item.name + '</div>';
        html += '<div class="chef-title-label">' + item.title + '</div>';
        html += '<div class="chef-bio">' + item.bio + '</div>';
        html += '<div class="chef-stats">';
        for (var key in item.stats) {
          html += '<div class="stat-row"><span class="stat-label">' + key + '</span>';
          html += '<span class="stat-stars" aria-label="' + item.stats[key] + ' out of 5">' + stars(item.stats[key]) + '</span></div>';
        }
        html += '</div>';
        infoEl.innerHTML = html;
      }

      render();
    })();

    // Chef Select
    (() => {
      const root = document.getElementById('chefSelect');
      if (!root) return;

      const motionOK = matchMedia('(prefers-reduced-motion: no-preference)').matches;

      const chefs = [
        {
          id: 'claude', name: 'Claude', title: 'The Head Chef of Clarity',
          img: '../images/character_select/Claude.png',
          bio: 'Runs a terrifyingly organized kitchen. Labels every jar, writes a prep list for the prep list, and politely refuses to substitute ingredients.<br><strong>Best for:</strong> careful reasoning, crisp writing, and code-review when you need structure.<br><strong>Watch for:</strong> occasionally trying to turn a simple grilled cheese into a masterpiece.',
          stats: { PREP: 5, FLAV: 2, HEAT: 1, GEAR: 4, PLAT: 4 }
        },
        {
          id: 'chatgpt', name: 'ChatGPT', title: 'The All-Purpose Chef',
          img: '../images/character_select/ChatGPT.png',
          bio: 'The reliable generalist who can run three stations and still answer your questions mid-rush.<br><strong>Best for:</strong> conversation, drafting, editing, explaining, brainstorming, and fast iteration when you want a steady back-and-forth.<br><strong>Watch for:</strong> occasionally substituting ingredients without saying anything.',
          stats: { PREP: 3, FLAV: 4, HEAT: 3, GEAR: 3, PLAT: 3 }
        },
        {
          id: 'gemini', name: 'Gemini', title: 'The Ambidextrous Caterer',
          img: '../images/character_select/Gemini.png',
          bio: 'Shows up with a banquet cart packed with cookbooks and various tricks of the trade.<br><strong>Best for:</strong> “here\u2019s a screenshot / table / doc\u2014tell me what matters,” research-y synthesis, and anything that mixes formats.<br><strong>Watch for:</strong> the occasional experimental garnish\u2014do an expo pass before it leaves the kitchen.',
          stats: { PREP: 4, FLAV: 2, HEAT: 3, GEAR: 5, PLAT: 2 }
        },
        {
          id: 'copilot', name: 'Copilot', title: 'The Prep Cook Living in the Pantry',
          img: '../images/character_select/CoPilot.png',
          bio: 'All the tools and always embedded in your IDE, your browser, your docs.  Not the flashiest chef on the line, but always ready to support with its signature move: auto-complete.<br><strong>Best for:</strong> staying in flow during your stay in Microsoft-land.<br><strong>Watch for:</strong> improvisation of the menu when overwhelmed.',
          stats: { PREP: 4, FLAV: 3, HEAT: 1, GEAR: 5, PLAT: 3 }
        },
        {
          id: 'grok', name: 'Grok', title: 'The Spicy Special',
          img: '../images/character_select/Grok.png',
          bio: 'Brings all the chaos of a herd of caffeinated squirrels in a bouncy castle wearing a trench coat. Also, an unapologetic fanboy of a certain billionaire, if that\u2019s at all important to the dish you\u2019re ordering.<br><strong>Best for:</strong> big swings and punchy rewrites when you want spicy ideas more than facts.<br><strong>Watch for:</strong> high variance\u2014it tends to over-season and improvise ingredients\u2014don\u2019t feed it after midnight.',
          stats: { PREP: 1, FLAV: 5, HEAT: 5, GEAR: 2, PLAT: 3 }
        },
        {
          id: 'llama', name: 'LLaMA', title: 'The Home Kitchen Kit',
          img: '../images/character_select/LLaMA.png',
          bio: 'Bring-your-own-stove: beloved for self-hosting, customization, and gourmet meals from the privacy of your own home.<br><strong>Best for:</strong> privacy-sensitive workflows and full control over tinkering, tuning, and swapping ingredients.<br><strong>Watch for:</strong> results vary by model/size and setup\u2014your mise en place (prompts, evals, settings, etc.) matters a lot.',
          stats: { PREP: 5, FLAV: 2, HEAT: 3, GEAR: 5, PLAT: 1 }
        }
      ];

      const rosterEl = document.getElementById('csRoster');
      const artEl = document.getElementById('csArt');
      const infoEl = document.getElementById('csInfo');
      const panelEl = document.getElementById('csPanel');
      const accentByChef = {
        claude: '#5f8fb0',
        chatgpt: '#629c67',
        gemini: '#9f79d4',
        copilot: '#4d7cc2',
        grok: '#cf6448',
        llama: '#7b8f4f'
      };
      let active = -1;
      let prevActive = -1;
      let swapTimer = 0;
      let accentTimer = 0;
      let hintTimer = 0;
      let hintShown = false;
      let lastScrollY = window.scrollY;
      let scrollTicking = false;

      function stars(n) {
        const on = '<span class="star-on">\u2605</span>'.repeat(n);
        const off = '<span class="star-off">\u2606</span>'.repeat(5 - n);
        return on + off;
      }

      function applyChefAccent() {
        if (active < 0) {
          root.style.removeProperty('--chef-accent');
          return;
        }
        const c = chefs[active];
        root.style.setProperty('--chef-accent', accentByChef[c.id] || 'var(--accent)');
      }

      function updatePanel() {
        if (active < 0) {
          if (panelEl) panelEl.classList.add('placeholder');
          artEl.innerHTML = '<img class="chef-art-placeholder" src="../images/character_select/Silhouette.png" alt="Generic chef silhouette">';
          infoEl.innerHTML =
            '<div class="chef-name">???</div>' +
            '<div class="chef-title-label">Select your chef</div>' +
            '<div class="chef-bio"></div>';
          return;
        }

        const c = chefs[active];
        if (panelEl) panelEl.classList.remove('placeholder');
        artEl.innerHTML = '<img class="chef-art-img" src="' + c.img + '" alt="' + c.name + ' chef card art">';

        let html = '<div class="chef-name">' + c.name + '</div>';
        html += '<div class="chef-title-label">' + c.title + '</div>';
        html += '<div class="chef-bio">' + c.bio + '</div>';
        html += '<div class="chef-stats">';
        for (const [key, val] of Object.entries(c.stats)) {
          html += '<div class="stat-row">';
          html += '<span class="stat-label">' + key + '</span>';
          html += '<span class="stat-stars" aria-label="' + val + ' out of 5">' + stars(val) + '</span>';
          html += '</div>';
        }
        html += '</div>';
        infoEl.innerHTML = html;
      }

      function revealStats() {
        if (!motionOK) return;
        infoEl.querySelectorAll('.stat-row').forEach((row, i) => {
          row.animate(
            [
              { opacity: 0, transform: 'translateX(-8px)' },
              { opacity: 1, transform: 'translateX(0)' }
            ],
            { duration: 250, delay: i * 60, easing: 'ease', fill: 'backwards' }
          );
        });
      }

      function animatePortraitSelection(buttonEl) {
        if (!motionOK || !buttonEl) return;
        buttonEl.animate(
          [
            { transform: 'scale(1)' },
            { transform: 'scale(1.08)' },
            { transform: 'scale(1)' }
          ],
          { duration: 300, easing: 'ease' }
        );
      }

      function showHintPulse() {
        if (!motionOK || hintShown || active >= 0) return;
        const target = rosterEl.querySelector('.portrait:not(.active)');
        if (!target) return;
        hintShown = true;
        target.classList.add('hint-pulse');
        target.addEventListener('animationend', () => {
          target.classList.remove('hint-pulse');
        }, { once: true });
      }

      function updateExitState() {
        const currentScrollY = window.scrollY;
        const deltaY = currentScrollY - lastScrollY;
        lastScrollY = currentScrollY;
        if (!root.classList.contains('in-view')) {
          root.classList.remove('exiting-up');
          scrollTicking = false;
          return;
        }
        const rect = root.getBoundingClientRect();
        const inViewZone = rect.top > window.innerHeight * 0.12 && rect.top < window.innerHeight * 1.05;
        const movingUpFast = deltaY < -14;
        root.classList.toggle('exiting-up', movingUpFast && inViewZone);
        scrollTicking = false;
      }

      function handleScroll() {
        if (scrollTicking) return;
        scrollTicking = true;
        requestAnimationFrame(updateExitState);
      }

      function render() {
        rosterEl.querySelectorAll('.portrait').forEach((p, i) => {
          p.classList.toggle('active', i === active && active >= 0);
        });

        const changed = active !== prevActive;
        prevActive = active;
        clearTimeout(swapTimer);
        clearTimeout(accentTimer);
        applyChefAccent();

        if (motionOK && changed) {
          artEl.classList.add('flipping');
          infoEl.classList.add('swapping');
          if (panelEl && active >= 0) {
            panelEl.classList.add('accent-pulse');
            accentTimer = setTimeout(() => {
              panelEl.classList.remove('accent-pulse');
            }, 260);
          }
          swapTimer = setTimeout(() => {
            updatePanel();
            requestAnimationFrame(() => {
              artEl.classList.remove('flipping');
              infoEl.classList.remove('swapping');
              revealStats();
            });
          }, 190);
        } else {
          updatePanel();
          if (changed) revealStats();
        }
      }

      const portraits = {
        claude: '../images/character_select/Claude-portrait.png',
        chatgpt: '../images/character_select/ChatGPT-portrait.png',
        gemini: '../images/character_select/Gemini-portrait.png',
        copilot: '../images/character_select/CoPilot-portrait.png',
        grok: '../images/character_select/Grok-portrait.png',
        llama: '../images/character_select/LLama-portrait.png'
      };

      chefs.forEach((c, i) => {
        const btn = document.createElement('button');
        btn.className = 'portrait';
        btn.type = 'button';
        btn.setAttribute('aria-label', 'Select ' + c.name);
        const img = document.createElement('img');
        img.src = portraits[c.id];
        img.alt = c.name;
        img.loading = 'lazy';
        btn.appendChild(img);
        const selectChef = () => {
          active = i;
          render();
          animatePortraitSelection(btn);
        };
        btn.addEventListener('click', selectChef);
        btn.addEventListener('keydown', (event) => {
          if (event.repeat) return;
          if (event.key !== 'Enter' && event.key !== ' ' && event.key !== 'Spacebar') return;
          event.preventDefault();
          selectChef();
        });
        rosterEl.appendChild(btn);
      });

      if (motionOK) {
        root.classList.add('motion-ready');
        const portraitEls = rosterEl.querySelectorAll('.portrait');
        portraitEls.forEach(p => { p.style.opacity = '0'; });

        const io = new IntersectionObserver(([entry]) => {
          if (entry.isIntersecting) {
            root.classList.add('in-view');
            portraitEls.forEach((p, i) => {
              p.animate(
                [
                  { opacity: 0, transform: 'translateY(12px)' },
                  { opacity: 1, transform: 'translateY(0)' }
                ],
                { duration: 400, delay: 50 + i * 60, easing: 'ease', fill: 'backwards' }
              );
              p.style.opacity = '';
            });
            hintTimer = setTimeout(showHintPulse, 520);
            io.disconnect();
          }
        }, { threshold: 0.15 });
        io.observe(root);
        window.addEventListener('scroll', handleScroll, { passive: true });
      }

      render();
    })();
  </script>
</body>

</html>
